{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing data interpolation\n",
    "\n",
    "statistics is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential shenanigans\n",
    "\n",
    "\"Several techniques have been used to fill the gaps in either the UWLS or OI derived total vector maps.\n",
    "\n",
    "These are implemented using covariance derived from normal mode analysis (Lipphardt et al. 2000), open-boundary modal analysis (OMA) (Kaplan and Lekien 2007), and empirical orthogonal function (EOF) analysis (Beckers and Rixen 2003; Alvera-Azc√°rate et al. 2005); and using idealized or smoothed observed covariance (Davis 1985).\"\n",
    "\n",
    "- normal mode analysis\n",
    "- open-boundary modal analysis (OMA)\n",
    "- empirical orthogonal function analysis (EOF)\n",
    "- use idealized/smoothed observed covariance\n",
    "\n",
    "---\n",
    "\n",
    "### other ideas\n",
    "\n",
    "DINEOF (could only find an implementation in R)\n",
    "\n",
    "to be honest I don't understand any of these methods but they look cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### currently implemented:\n",
    "\n",
    "rip data straight from the lower resolution data for areas where data is considered missing in the high resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from parcels import FieldSet\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import utils\n",
    "from parcels_utils import get_file_info, xr_dataset_to_fieldset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target and interp_references\n",
    "\n",
    "`target` is the data you are interpolating.\n",
    "\n",
    "`interp_references` is a list of reference data to interpolate from. A few specifications:\n",
    "- should be ordered from most accurate data to least accurate (highest to lowest resolution)\n",
    "- time domain should be identical or bigger than the one of the target\n",
    "- lat and lon domain should be bigger than the target's to prevent any out-of-bounds complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = get_file_info(utils.CURRENT_NETCDF_DIR / \"west_coast_1km_hourly/tijuana_river.nc\", utils.DATA_1KM, name=\"target\")\n",
    "\n",
    "interp_references = [\n",
    "    get_file_info(utils.CURRENT_NETCDF_DIR / \"west_coast_2km_hourly/tijuana_river.nc\", utils.DATA_2KM, name=\"ref2km\"),\n",
    "    get_file_info(utils.CURRENT_NETCDF_DIR / \"west_coast_6km_hourly/tijuana_river.nc\", utils.DATA_6KM, name=\"ref6km\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check validity of interpolation references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_min = (target[\"lat\"][0], target[\"lon\"][0])\n",
    "targ_max = (target[\"lat\"][-1], target[\"lon\"][-1])\n",
    "for ir in interp_references:\n",
    "    print(f\"checking {ir['name']}\")\n",
    "    assert ir[\"lat\"][0] <= targ_min[0]\n",
    "    assert ir[\"lon\"][0] <= targ_min[1]\n",
    "    assert ir[\"lat\"][-1] >= targ_max[0]\n",
    "    assert ir[\"lon\"][-1] >= targ_max[1]\n",
    "    assert ir[\"timerng\"][0] <= target[\"timerng\"][0]\n",
    "    assert ir[\"timerng\"][1] >= target[\"timerng\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolation type\n",
    "\n",
    "more information can be found in the `tutorial_interpolation` notebook\n",
    "\n",
    "EDIT: just use `linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_interp_method = \"linear\"\n",
    "\n",
    "for r in interp_references:\n",
    "    r[\"fs_flat\"].U.interp_method = reference_interp_method\n",
    "    r[\"fs_flat\"].V.interp_method = reference_interp_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan values and parcels\n",
    "\n",
    "note that when this xarray Dataset is passed into parcels, all the nan values change to 0 and the mask generation won't work anymore\n",
    "\n",
    "so the Dataset is copied for use with the FieldSet instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_data = utils.generate_mask_none(target[\"xrds\"][\"u\"].values)\n",
    "invalid = utils.generate_mask_invalid(target[\"xrds\"][\"u\"].values)\n",
    "invalid_pos = np.where(invalid)\n",
    "num_invalid = invalid.sum()\n",
    "print(f\"total invalid values on target data: {num_invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use of Parcels Field for interpolation\n",
    "\n",
    "indexing Field values goes [time, depth, lat, lon]\n",
    "\n",
    "Field does linear interpolation automatically when indexing values between it's coordinate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note\n",
    "\n",
    "https://stackoverflow.com/questions/12923586/nearest-neighbor-search-python\n",
    "\n",
    "in theory, the latitude and longitude values are equally spaced. however, the difference between the coordinate values always flucuates a very small amount between two distinct values, so it's not perfectly equally spaced.\n",
    "\n",
    "from testing, this causes enough error to completely change a simulation, so a kdtree must be used.\n",
    "\n",
    "### another note\n",
    "\n",
    "I just looked at the particle trajectory that used the indexing without kd trees and it looked MAJORLY fucked up, so I probably wrote something wrong with the method. TODO will come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "# set up kdtrees for all references\n",
    "for f in interp_references:\n",
    "    f[\"latkdtree\"] = scipy.spatial.cKDTree(np.array([f[\"lat\"]]).T)\n",
    "    f[\"lonkdtree\"] = scipy.spatial.cKDTree(np.array([f[\"lon\"]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_current(ref, t, lat, lon):\n",
    "    index = (t, ref[\"latkdtree\"].query([lat])[1], ref[\"lonkdtree\"].query([lon])[1])\n",
    "    return ref[\"xrds\"][\"u\"].values[index], ref[\"xrds\"][\"v\"].values[index]\n",
    "\n",
    "\n",
    "def get_current_interp(ref, t, lat, lon):\n",
    "    return ref[\"fs_flat\"].U[t, 0, lat, lon], ref[\"fs_flat\"].V[t, 0, lat, lon]\n",
    "\n",
    "\n",
    "def get_interped(i, ref, invalid_where):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        i (int): index on invalid_where\n",
    "        ref (dict): reference Dataset\n",
    "        invalid_where (array-like): (3, n) dimensional array representing all invalid positions\n",
    "    \n",
    "    Returns:\n",
    "        (u, v): (nan, nan) if no data was found, interpolated values otherwise\n",
    "    \"\"\"\n",
    "    time_diff = np.diff(ref[\"fs_flat\"].U.grid.time)[0]\n",
    "    t = invalid_where[0][i]\n",
    "    lat = target[\"lat\"][invalid_where[1][i]]\n",
    "    lon = target[\"lon\"][invalid_where[2][i]]\n",
    "    current_u, current_v = get_current_interp(ref, t * time_diff, lat, lon)\n",
    "    current_abs = abs(current_u) + abs(current_v)\n",
    "    # if both the u and v components are 0, there's probably no data there\n",
    "    if np.isnan(get_nearest_current(ref, t, lat, lon)[0]) or current_abs == 0:\n",
    "        return np.nan, np.nan\n",
    "    return current_u, current_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear interpolation using lower resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_interped_u = target[\"xrds\"][\"u\"].values.copy()\n",
    "target_interped_v = target[\"xrds\"][\"v\"].values.copy()\n",
    "invalid_interped = invalid.copy()\n",
    "for f in interp_references:\n",
    "    invalid_pos_new = np.where(invalid_interped)\n",
    "    num_invalid_new = invalid_interped.sum()\n",
    "    arr_u = np.zeros(num_invalid_new)\n",
    "    arr_v = np.zeros(num_invalid_new)\n",
    "    for i in range(num_invalid_new):\n",
    "        c_u, c_v = get_interped(i, f, invalid_pos_new)\n",
    "        arr_u[i] = c_u\n",
    "        arr_v[i] = c_v\n",
    "    target_interped_u[invalid_pos_new] = arr_u\n",
    "    target_interped_v[invalid_pos_new] = arr_v\n",
    "    invalid_interped = utils.generate_mask_invalid(target_interped_u)\n",
    "    print(f\"total invalid values after interpolation with {f['name']}: {invalid_interped.sum()}\")\n",
    "    print(f\"    values filled: {num_invalid_new - invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total invalid values on interpolated: {invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: even more filling with PLS and smoothing with DCT shenanigans\n",
    "\n",
    "this is just one option, but hey\n",
    "\n",
    "ps ignore this code i have no idea what i was doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fftpack\n",
    "\n",
    "# temporary testing with smoothing\n",
    "# i just brainlessly stuck the equations in here\n",
    "field_shape = target_interped_u.shape\n",
    "k, l = np.meshgrid(np.arange(field_shape[1]), np.arange(field_shape[2]), indexing='ij')\n",
    "s = 0.1\n",
    "\n",
    "lam = 4 - 2 * np.cos((k - 1) * math.pi / field_shape[1]) - 2 * np.cos((l - 1) * math.pi / field_shape[2])\n",
    "gam = 1 / (1 + s * lam ** 2)\n",
    "\n",
    "# dct freaks out when there's nans, have to replace them with 0\n",
    "smoothed_u = np.nan_to_num(target_interped_u)\n",
    "smoothed_v = np.nan_to_num(target_interped_v)\n",
    "for i in range(smoothed_u.shape[0]):\n",
    "    smoothed_u[i] = scipy.fftpack.idct(gam * scipy.fftpack.dct(smoothed_u[i], norm=\"ortho\"), norm=\"ortho\")\n",
    "    smoothed_v[i] = scipy.fftpack.idct(gam * scipy.fftpack.dct(smoothed_v[i], norm=\"ortho\"), norm=\"ortho\")\n",
    "smoothed_u[np.where(no_data)] = np.nan\n",
    "smoothed_v[np.where(no_data)] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gridfill invalid values after linear interpolation\n",
    "\n",
    "in theory there should still be invalid spaces left over because the 6 km data will sometimes have gaps\n",
    "\n",
    "hopefully after this interpolation, the gaps of invalid data are small enough to let gridfilling finish the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy.ma as ma\n",
    "# from gridfill import fill\n",
    "\n",
    "# masked_u = ma.masked_array(target_interped_u, invalid_interped)\n",
    "# masked_v = ma.masked_array(target_interped_v, invalid_interped)\n",
    "\n",
    "# kw = dict(eps=1e-4, relax=0.6, itermax=1e4, initzonal=False,\n",
    "#           cyclic=False, verbose=False)\n",
    "\n",
    "# # since data is 3d, use axes 2 and 1 since axis 0 is time\n",
    "# filled_u, converged_u = fill(masked_u, 2, 1, **kw)\n",
    "# filled_v, converged_v = fill(masked_v, 2, 1, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invalid_filled = generate_mask_invalid(filled_u)\n",
    "# print(f\"total invalid values on filled: {invalid_filled.sum()}\")\n",
    "# print(f\"percent invalid filled: {(invalid_interped.sum() - invalid_filled.sum()) / invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_u = target_interped_u\n",
    "filled_v = target_interped_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wow\n",
    "\n",
    "unsurprisingly gridfill did jack shit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formatting and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-add coordinates, dimensions, and metadata to interpolated data\n",
    "darr_u = utils.conv_to_dataarray(filled_u, target[\"xrds\"][\"u\"])\n",
    "darr_v = utils.conv_to_dataarray(filled_v, target[\"xrds\"][\"v\"])\n",
    "target_interped_xrds = target[\"xrds\"].drop_vars([\"u\", \"v\"]).assign(u=darr_u, v=darr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-add coordinates, dimensions, and metadata to smoothed data\n",
    "darr_u_smooth = utils.conv_to_dataarray(smoothed_u, target[\"xrds\"][\"u\"])\n",
    "darr_v_smooth = utils.conv_to_dataarray(smoothed_v, target[\"xrds\"][\"v\"])\n",
    "target_interped_xrds_smooth = target[\"xrds\"].drop_vars([\"u\", \"v\"]).assign(u=darr_u_smooth, v=darr_v_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = str(target[\"path\"]).split(\".nc\")[0] + \"_interped.nc\"\n",
    "target_interped_xrds.to_netcdf(save_path)\n",
    "print(f\"saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display field to see if interpolation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_interp = xr_dataset_to_fieldset(target_interped_xrds)\n",
    "fs_smooth = xr_dataset_to_fieldset(target_interped_xrds_smooth)\n",
    "target[\"fs\"].U.show()  # uninterpolated\n",
    "fs_interp.U.show()  # interpolated\n",
    "fs_smooth.U.show()  # interpolated + smooth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
