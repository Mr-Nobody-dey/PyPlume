{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parcels regions\n",
    "\n",
    "runs parcels on existing netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from parcels import FieldSet, ParticleSet\n",
    "from datetime import timedelta\n",
    "\n",
    "import utils\n",
    "from utils import load_config, create_path\n",
    "from parcels_utils import get_file_info\n",
    "from plot_utils import plot_particles_age\n",
    "\n",
    "# ignore annoying deprecation warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import cartopy\n",
    "\n",
    "# ignore divide by nan error that happens constantly with parcels\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration\n",
    "\n",
    "change the contents of `configs` for the simulation configuration you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "#     \"parcels_configs/tijuana_lin.json\",\n",
    "#     \"parcels_configs/tijuana_simpterp.json\",\n",
    "#     \"parcels_configs/tijuana_lin_aggr.json\",\n",
    "    \"parcels_configs/tijuana_less.json\",\n",
    "#     \"parcels_configs/tijuana_small.json\",\n",
    "#     \"parcels_configs/tijuana_smallreally.json\"\n",
    "]\n",
    "\n",
    "loaded_configs = [load_config(path) for path in configs]\n",
    "files = [get_file_info(cfg[\"netcdf_path\"], cfg[\"resolution\"], name=cfg[\"name\"], parcels_cfg=cfg[\"parcels_config\"]) for cfg in loaded_configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated gif stuff and particle simulation\n",
    "\n",
    "runs on each file you give it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation man very cool\n",
    "# reference tutorial_Agulhasparticles\n",
    "# needs ErrorCode for particle recovery\n",
    "from operator import attrgetter\n",
    "from parcels import ErrorCode, JITParticle, Variable, AdvectionRK4\n",
    "\n",
    "max_v = 1.1 # for display purposes only, so the vector field colors don't change every iteration\n",
    "\n",
    "class TimedParticle(JITParticle):\n",
    "    lifetime = Variable(\"lifetime\", initial=0, dtype=np.float32)\n",
    "    spawntime = Variable(\"spawntime\", initial=attrgetter(\"time\"), dtype=np.float32)\n",
    "\n",
    "    \n",
    "def ParticleLifetime(particle, fieldset, time):\n",
    "    particle.lifetime += particle.dt\n",
    "\n",
    "    \n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()\n",
    "\n",
    "    \n",
    "def exec_save_pset(data, i, runtime, dt, zpad=3, ages=True, save_snapshot=True, exec_pset=True):\n",
    "    \"\"\"\n",
    "    Saves a snapshot of a particle simulation and then executes.\n",
    "    \n",
    "    Args:\n",
    "        data (dict)\n",
    "        i (int)\n",
    "        runtime (float): seconds\n",
    "        dt (float): seconds\n",
    "    \"\"\"\n",
    "    if save_snapshot:\n",
    "        # TODO add option if you want to see particle ages or just the vector field\n",
    "        # because right now can't do both\n",
    "        days = (data[\"timerng\"][1] - data[\"timerng\"][0]) / np.timedelta64(1, 'D')\n",
    "#         data[\"pset\"].show(savefile=str(data[\"snap_path\"])+\"/particles\"+str(i).zfill(zpad), field=\"vector\", vmax=max_v)\n",
    "        field = None if ages else \"vector\"\n",
    "        if \"shown_domain\" in data[\"cfg\"]:\n",
    "            dom = data[\"cfg\"][\"shown_domain\"]\n",
    "        else:\n",
    "            dom = data[\"domain\"]\n",
    "        plot_particles_age(data[\"pset\"], dom, field=field,\n",
    "                           savefile=str(data[\"snap_path\"])+\"/particles\"+str(i).zfill(zpad),\n",
    "                           vmax=days, field_vmax=max_v)\n",
    "    \n",
    "    if exec_pset:\n",
    "        # temporary - TODO make it only init once\n",
    "        k_plifetime = data[\"pset\"].Kernel(ParticleLifetime)\n",
    "\n",
    "        data[\"pset\"].execute(\n",
    "            AdvectionRK4 + k_plifetime,\n",
    "            runtime=timedelta(seconds=runtime),\n",
    "            dt=timedelta(seconds=dt),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle},\n",
    "            output_file=data[\"pfile\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParticleSet and spawn point setup\n",
    "\n",
    "note about interpolation methods: only `linear` works if you want to use the FieldSet in a ParticleSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "part_path = create_path(\"particledata\")\n",
    "\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    repeat_dt = timedelta(seconds=cfg[\"repeat_dt\"]) # interval at which particles are released\n",
    "    cfg[\"spawn_points\"] = np.array(cfg[\"spawn_points\"]) # particle spawns will be randomly chosen between these points\n",
    "    \n",
    "    repetitions = math.floor(f[\"timerng_secs\"][1] / repeat_dt.total_seconds())\n",
    "    # the total number of particles that will exist in the simulation\n",
    "    total = repetitions * cfg[\"particles_per_dt\"]\n",
    "    lat_arr = np.zeros(total)\n",
    "    lon_arr = np.zeros(total)\n",
    "    time_arr = np.zeros(total)\n",
    "    for i in range(repetitions):\n",
    "        time_arr[cfg[\"particles_per_dt\"] * i:cfg[\"particles_per_dt\"] * (i + 1)] = repeat_dt.total_seconds() * i\n",
    "\n",
    "    # randomly select spawn points from the given config\n",
    "    sp_lat = cfg[\"spawn_points\"].T[0][np.random.randint(0, len(cfg[\"spawn_points\"]), total)]\n",
    "    sp_lon = cfg[\"spawn_points\"].T[1][np.random.randint(0, len(cfg[\"spawn_points\"]), total)]\n",
    "    # vary spawn locations using max_variation\n",
    "    variances_lat = (np.random.random(total) * 2 - 1) * cfg[\"max_variation\"]\n",
    "    variances_lon = (np.random.random(total) * 2 - 1) * cfg[\"max_variation\"]\n",
    "\n",
    "    p_lats = sp_lat + variances_lat\n",
    "    p_lons = sp_lon + variances_lon\n",
    "\n",
    "    # set up ParticleSet and ParticleFile\n",
    "    f[\"pset\"] = ParticleSet(fieldset=f[\"fs\"], pclass=TimedParticle, lon=p_lons, lat=p_lats, time=time_arr)\n",
    "    save_path = f\"{part_path}/particle_{f['name']}.nc\"\n",
    "    f[\"pfile\"] = f[\"pset\"].ParticleFile(save_path)\n",
    "    print(f\"Particle trajectories for {f['name']} will be saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f[\"pset\"].show(field=\"vector\", vmax=max_v) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulation setup and execution\n",
    "\n",
    "simulation parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting up times, intervals, and paths for simulation\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    cfg[\"snap_num\"] = math.floor((f[\"timerng_secs\"][1] - f[\"timerng_secs\"][0]) / cfg[\"snapshot_interval\"])\n",
    "    cfg[\"last_int\"] = f[\"timerng_secs\"][1] - cfg[\"snap_num\"] * cfg[\"snapshot_interval\"]\n",
    "    if cfg[\"last_int\"] == 0:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {cfg['snap_num'] + 1}\")\n",
    "    else:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {cfg['snap_num'] + 2}\")\n",
    "    f[\"snap_path\"] = create_path(f\"snapshots/{utils.filename_dict[f['res']]}/{f['name']}\")\n",
    "    print(f\"Path to save snapshots to: {f['snap_path']}\")\n",
    "    # only clear directory if desired or actually saving images\n",
    "    if cfg[\"save_snapshots\"]:\n",
    "        for p in f[\"snap_path\"].glob(\"*.png\"):\n",
    "            p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execution of all simulation configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show age or not (temporary)\n",
    "age = False\n",
    "# execution of simulation\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    for i in range(cfg[\"snap_num\"]):\n",
    "        exec_save_pset(f, i, cfg[\"snapshot_interval\"], cfg[\"simulation_dt\"], ages=age, save_snapshot=cfg[\"save_snapshots\"])\n",
    "\n",
    "    # save the second-to-last frame\n",
    "    exec_save_pset(f, cfg[\"snap_num\"], 0, 0, ages=age, save_snapshot=cfg[\"save_snapshots\"], exec_pset=False)\n",
    "\n",
    "    # run the last interval (the remainder) if needed\n",
    "    if cfg[\"last_int\"] != 0:\n",
    "        exec_save_pset(f, cfg[\"snap_num\"] + 1, cfg[\"last_int\"], cfg[\"simulation_dt\"], ages=age, save_snapshot=cfg[\"save_snapshots\"])\n",
    "        \n",
    "    # run one more time with 0 runtime because for some reason\n",
    "    # particles can be out-of-bounds without being deleted\n",
    "    # ??? how does this happen ???\n",
    "    # im not actually sure if this really does anything but hey its worth a shot\n",
    "    exec_save_pset(f, 0, 0, 0, save_snapshot=False)\n",
    "        \n",
    "    f[\"pfile\"].export()\n",
    "    f[\"pfile\"].close()\n",
    "\n",
    "print(\"all simulations done and snapshots saved (if simulation was saving snapshots)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gif generation\n",
    "\n",
    "don't have to run, requires [magick](https://imagemagick.org/index.php)\n",
    "\n",
    "the gifs will be saved `snapshots/west_coast_xkm_hourly/` where xkm is the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "gif_delay = 25 # ms\n",
    "\n",
    "for f in files:\n",
    "    if f[\"cfg\"][\"save_snapshots\"]:\n",
    "        magick_sp = subprocess.Popen(\n",
    "            [\n",
    "                \"magick\", \"-delay\", str(gif_delay),\n",
    "                str(f[\"snap_path\"]) + \"/*.png\", # path to the snapshots to stitch\n",
    "                f\"snapshots/{utils.filename_dict[f['res']]}/partsim_{f['name']}.gif\" # path to save gif to\n",
    "            ],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        stdout, stderr = magick_sp.communicate()\n",
    "        print((stdout, stderr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
