{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate_regions_auto\n",
    "\n",
    "this notebook's purpose is to automatically find regions of interest in the ocean current maps and slice them out\n",
    "\n",
    "in other words, find the regions with actual data and get some info about the coordinates\n",
    "\n",
    "then pickle it to use the region data in `access_thredds` notebook\n",
    "\n",
    "### note\n",
    "\n",
    "to generate a pickle for a different region, change the `dataset_url` variable and rerun the whole notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from parcels import FieldSet\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url_6kmhourly = \"http://hfrnet-tds.ucsd.edu/thredds/dodsC/HFR/USWC/6km/hourly/RTV/HFRADAR_US_West_Coast_6km_Resolution_Hourly_RTV_best.ncd\"\n",
    "dataset_url_2kmhourly = \"http://hfrnet-tds.ucsd.edu/thredds/dodsC/HFR/USWC/2km/hourly/RTV/HFRADAR_US_West_Coast_2km_Resolution_Hourly_RTV_best.ncd\"\n",
    "dataset_url_1kmhourly = \"http://hfrnet-tds.ucsd.edu/thredds/dodsC/HFR/USWC/1km/hourly/RTV/HFRADAR_US_West_Coast_1km_Resolution_Hourly_RTV_best.ncd\"\n",
    "filename_dict = {\n",
    "    dataset_url_1kmhourly: \"west_coast_1km_hourly\",\n",
    "    dataset_url_2kmhourly: \"west_coast_2km_hourly\",\n",
    "    dataset_url_6kmhourly: \"west_coast_6km_hourly\"\n",
    "}\n",
    "\n",
    "### CHANGE THIS TO CHANGE THE DATA RETRIEVED ###\n",
    "dataset_url = dataset_url_2kmhourly\n",
    "### ######################################## ###\n",
    "\n",
    "time_chunk_size = 50\n",
    "chunk_dict = {\"time\": time_chunk_size}\n",
    "to_drop = [\"dopx\", \"dopy\", \"hdop\", \"number_of_sites\", \"number_of_radials\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_xr = xr.open_dataset(dataset_url, chunks=chunk_dict, drop_variables=to_drop)\n",
    "fs_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sampling data at multiple times\n",
    "\n",
    "for some hours of data, parts of the data aren't recorded or available to avoid forgetting about the lost data, we check multiple hours of data vs just using the latest data as reference\n",
    "\n",
    "on the contrary, if some important radar went offline for a particularly long time, this won't work well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timespan_to_sample = np.timedelta64(15,'h')\n",
    "time_end = fs_xr[\"time\"].max()\n",
    "time_start = time_end - timespan_to_sample\n",
    "fs_xr_slice = fs_xr.sel(time=slice(time_start, time_end))\n",
    "fs_xr_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_diff = float(fs_xr_slice[\"lat\"].diff(dim=\"lat\").max().values)\n",
    "lon_diff = float(fs_xr_slice[\"lon\"].diff(dim=\"lon\").max().values)\n",
    "print(lat_diff, lon_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounding the stuff is fine since xarray does nearest neighbor lookups\n",
    "lat_range = (math.floor(fs_xr_slice[\"lat\"].min()), math.ceil(fs_xr_slice[\"lat\"].max()))\n",
    "lon_range = (math.floor(fs_xr_slice[\"lon\"].min()), math.ceil(fs_xr_slice[\"lon\"].max()))\n",
    "print(lat_range, lon_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finding areas of interest\n",
    "\n",
    "a quickly conceived method of continuously dividing the map smaller and smaller\n",
    "\n",
    "the more i look at the code the more jank it becomes\n",
    "\n",
    "the only reason this works is because generally there's only one region of data in a latitudal region\n",
    "\n",
    "this shit takes ages to run so grab a drink (up to five minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for adjacent regions found to be of interest, merge them together into one\n",
    "def merge_regions(regs, diff, num_divs):\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(regs):\n",
    "        start = regs[i]\n",
    "        count = 1\n",
    "        # region is as small as possible, don't bother merging\n",
    "        # reached end of list, last element won't merge\n",
    "        if start[2] or i >= len(regs) - 1:\n",
    "            end = start\n",
    "        else:\n",
    "            while regs[i + 1][0] - regs[i][1] <= diff + sys.float_info.epsilon:\n",
    "                count += 1\n",
    "                i += 1\n",
    "                # reached end of list, break\n",
    "                if i == len(regs) - 1:\n",
    "                    break\n",
    "            end = regs[i]\n",
    "        # the number of regions being merged equals the number of divisions\n",
    "        # this means it cannot be divided further\n",
    "        if start[2] or count == num_divs:\n",
    "            merged.append((start[0], end[1], True))\n",
    "        else:\n",
    "            merged.append((start[0], end[1], False))\n",
    "        i += 1\n",
    "    print(merged)\n",
    "    print(\"------------------------\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of regions to divide the map into\n",
    "# higher values are better if the regions are closer together\n",
    "lat_divs = 18\n",
    "lon_divs = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# dividing latitude regions of interest\n",
    "# regions have a flag as the 3rd element marking whether they are the smallest possible or not\n",
    "# at the end of the algorithm, the flags won't necessarily be all True\n",
    "lat_regions = [(lat_range[0], lat_range[1], False)]\n",
    "lat_found = False\n",
    "while not lat_found:\n",
    "    prev_regions = lat_regions.copy()\n",
    "    # clear regions to find new regions of interest\n",
    "    lat_regions = []\n",
    "    for r in prev_regions:\n",
    "        if r[2]:\n",
    "            lat_regions.append(r)\n",
    "            continue\n",
    "        # lat_divs + 1 or else we only get lat_divs - 1 regions\n",
    "        r_secs = np.linspace(r[0], r[1], lat_divs + 1)\n",
    "        for i in range(0, lat_divs):\n",
    "            xr_slice = fs_xr_slice.sel(lat=slice(r_secs[i], r_secs[i + 1]), lon=slice(lon_range[0], lon_range[1]))\n",
    "            # check if region actually has data\n",
    "            # the main bottleneck, this check takes so long to run\n",
    "            if not np.isnan(xr_slice[\"u\"]).all().values:\n",
    "                # append latitude values according to the coordinate values of the data\n",
    "                lat_regions.append((float(xr_slice[\"lat\"].min().values), float(xr_slice[\"lat\"].max().values), False))\n",
    "    lat_regions = merge_regions(lat_regions, lat_diff, lat_divs)\n",
    "    # this check should be fine since it takes coordinates straight from the data itself\n",
    "    if lat_regions == prev_regions:\n",
    "        lat_found = True\n",
    "end = time.time()\n",
    "print(f\"time elapsed: {end - start}\")\n",
    "lat_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HFR 6km 52.875375270843506 seconds\n",
    "\n",
    "HFR 2km 163.83207821846008 seconds w/o lat lon chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# dividing each latitude region into longitude regions\n",
    "# stored as a list of lists\n",
    "lon_regions = [[(lon_range[0], lon_range[1], False)]] * len(lat_regions)\n",
    "lon_found = False\n",
    "while not lon_found:\n",
    "    # deep copy list because it has lists inside\n",
    "    prev_regions = copy.deepcopy(lon_regions)\n",
    "    for s in range(0, len(lat_regions)):\n",
    "        lon_regions[s] = []\n",
    "        for r in prev_regions[s]:\n",
    "            if r[2]:\n",
    "                lon_regions[s].append(r)\n",
    "                continue\n",
    "            r_secs = np.linspace(r[0], r[1], lon_divs + 1)\n",
    "            for i in range(0, lon_divs):\n",
    "                xr_slice = fs_xr_slice.sel(lat=slice(lat_regions[s][0], lat_regions[s][1]), lon=slice(r_secs[i], r_secs[i + 1]))\n",
    "                if not np.isnan(xr_slice[\"u\"]).all().values:\n",
    "                    lon_regions[s].append((float(xr_slice[\"lon\"].min().values), float(xr_slice[\"lon\"].max().values), False))\n",
    "    for i in range(0, len(lon_regions)):\n",
    "        lon_regions[i] = merge_regions(lon_regions[i], lon_diff, lon_divs)\n",
    "    if lon_regions == prev_regions:\n",
    "        lon_found = True\n",
    "end = time.time()\n",
    "print(f\"time elapsed: {end - start}\")\n",
    "lon_regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the pickle dump\n",
    "\n",
    "where the data does dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = []\n",
    "for i in range(0, len(lat_regions)):\n",
    "    for lonr in lon_regions[i]:\n",
    "        regs.append((lat_regions[i], lonr))\n",
    "d = filename_dict[dataset_url]\n",
    "with open(f\"regions_{d}.p\", \"wb\") as f:\n",
    "    pickle.dump(regs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots of regions found\n",
    "\n",
    "mostly for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a plot of all the regions of interest found\n",
    "for i in range(0, len(lat_regions)):\n",
    "    for lonr in lon_regions[i]:\n",
    "        xarr_sliced = fs_xr_slice.sel(lat=slice(lat_regions[i][0], lat_regions[i][1]), lon=slice(lonr[0], lonr[1]))\n",
    "        print((math.floor(xarr_sliced[\"lat\"].min()), math.ceil(xarr_sliced[\"lat\"].max())))\n",
    "    #     print(xarr_sliced[\"u\"].sum())\n",
    "        a = xarr_sliced[\"u\"].sum().values\n",
    "        print(f\"sum U: {a}\")\n",
    "        fs_sliced = FieldSet.from_xarray_dataset(xarr_sliced, dict(U=\"u\",V=\"v\"), dict(lat=\"lat\",lon=\"lon\",time=\"time\"))\n",
    "        fs_sliced.U.show(vmin=-1, vmax=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
