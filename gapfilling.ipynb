{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing data interpolation\n",
    "\n",
    "statistics is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential shenanigans\n",
    "\n",
    "\"Several techniques have been used to fill the gaps in either the UWLS or OI derived total vector maps.\n",
    "\n",
    "These are implemented using covariance derived from normal mode analysis (Lipphardt et al. 2000), open-boundary modal analysis (OMA) (Kaplan and Lekien 2007), and empirical orthogonal function (EOF) analysis (Beckers and Rixen 2003; Alvera-Azc√°rate et al. 2005); and using idealized or smoothed observed covariance (Davis 1985).\"\n",
    "\n",
    "- normal mode analysis\n",
    "- open-boundary modal analysis (OMA)\n",
    "- empirical orthogonal function analysis (EOF)\n",
    "- use idealized/smoothed observed covariance\n",
    "\n",
    "---\n",
    "\n",
    "### other ideas\n",
    "\n",
    "DINEOF (could only find an implementation in R)\n",
    "\n",
    "to be honest I don't understand any of these methods but they look cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### currently implemented:\n",
    "\n",
    "rip data straight from the lower resolution data for areas where data is considered missing in the high resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from parcels import FieldSet\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from utils import get_file_info, generate_mask, conv_to_dataarray, xr_dataset_to_fieldset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target and interp_references\n",
    "\n",
    "`target` is the data you are interpolating.\n",
    "\n",
    "`interp_references` is a list of reference data to interpolate from. A few specifications:\n",
    "- should be ordered from most accurate data to least accurate\n",
    "- time domain should be identical or bigger than the one of the target\n",
    "- lat and lon domain should be bigger than the target's to prevent any out-of-bounds complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = get_file_info(\"target\", \"west_coast_1km_hourly_netcdfs/west_coast_1km_hourly_region_tijuana_river.nc\", 1)\n",
    "\n",
    "interp_references = [\n",
    "    get_file_info(\"ref2km\", \"west_coast_2km_hourly_netcdfs/west_coast_2km_hourly_region0.nc\", 2),\n",
    "    get_file_info(\"ref6km\", \"west_coast_6km_hourly_netcdfs/west_coast_6km_hourly_region0.nc\", 6)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolation type\n",
    "\n",
    "more information can be found in the `tutorial_interpolation` notebook\n",
    "\n",
    "it's mostly finding whether `linear` or `linear_invdist_land_tracer` works better\n",
    "\n",
    "FUN FACT: parcels lets you change the interp method for U and V fields to `linear_invdist_land_tracer` and even access values from it, but doesn't work when you stick the FieldSet into a ParticleSet.\n",
    "\n",
    "maybe this means `linear_invdist_land_tracer` shouldn't be used for velocity interpolation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_interp_method = \"linear\"\n",
    "# reference_interp_method = \"linear_invdist_land_tracer\"\n",
    "\n",
    "for r in interp_references:\n",
    "    r[\"fs_flat\"].U.interp_method = reference_interp_method\n",
    "    r[\"fs_flat\"].V.interp_method = reference_interp_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan values and parcels\n",
    "\n",
    "note that when this xarray Dataset is passed into parcels, all the nan values change to 0 and the mask generation won't work anymore\n",
    "\n",
    "so the Dataset is copied for use with the FieldSet instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid = generate_mask(target[\"xrds\"][\"u\"].values)\n",
    "num_invalid = invalid.sum()\n",
    "print(f\"total invalid values on target data: {num_invalid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_pos = np.where(invalid)\n",
    "invalid_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use of Parcels Field for interpolation\n",
    "\n",
    "indexing Field values goes [time, depth, lat, lon]\n",
    "\n",
    "Field does interpolation automatically when indexing values between it's coordinate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note\n",
    "\n",
    "https://stackoverflow.com/questions/12923586/nearest-neighbor-search-python\n",
    "\n",
    "in theory, the latitude and longitude values are equally spaced. however, the difference between the coordinate values always flucuates a very small amount between two distinct values, so it's not perfectly equally spaced.\n",
    "\n",
    "from testing, this causes enough error to completely change a simulation, so a kdtree must be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "for f in interp_references:\n",
    "    f[\"latkdtree\"] = scipy.spatial.cKDTree(np.array([f[\"lat\"]]).T)\n",
    "    f[\"lonkdtree\"] = scipy.spatial.cKDTree(np.array([f[\"lon\"]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interped(i, ref, vec, invalid_where):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        i (int)\n",
    "        ref (dict): reference Dataset\n",
    "        vec (str): either u or v\n",
    "        invalid_where (array-like)\n",
    "    \n",
    "    Returns:\n",
    "        float: nan if no data was found, interpolated value otherwise\n",
    "    \"\"\"\n",
    "    time_diff = np.diff(ref[\"fs_flat\"].U.grid.time)[0]\n",
    "    t = invalid_where[0][i]\n",
    "    lat = target[\"lat\"][invalid_where[1][i]]\n",
    "    lon = target[\"lon\"][invalid_where[2][i]] \n",
    "    if np.isnan(ref[\"xrds\"][vec].values[t, ref[\"latkdtree\"].query([lat])[1], ref[\"lonkdtree\"].query([lon])[1]]):\n",
    "        return np.nan\n",
    "    if vec == \"u\":\n",
    "        return ref[\"fs_flat\"].U[t * time_diff, 0, lat, lon]\n",
    "    return ref[\"fs_flat\"].V[t * time_diff, 0, lat, lon]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### today i learned\n",
    "\n",
    "doing DataArray.values does not return a numpy array copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_st = time.time()\n",
    "target_interped_u = target[\"xrds\"][\"u\"].values.copy()\n",
    "target_interped_v = target[\"xrds\"][\"v\"].values.copy()\n",
    "invalid_interped = invalid.copy()\n",
    "for f in interp_references:\n",
    "    invalid_pos_new = np.where(invalid_interped)\n",
    "    num_invalid_new = invalid_interped.sum()\n",
    "    target_interped_u[invalid_pos_new] = [get_interped(i, f, \"u\", invalid_pos_new) for i in range(num_invalid_new)]\n",
    "    target_interped_v[invalid_pos_new] = [get_interped(i, f, \"v\", invalid_pos_new) for i in range(num_invalid_new)]\n",
    "    invalid_interped = generate_mask(target_interped_u)\n",
    "time_en = time.time()\n",
    "print(f\"time elapsed: {time_en - time_st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total invalid values on interpolated: {invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gridfill invalid values after interpolation\n",
    "\n",
    "in theory there should still be invalid spaces left over because the 6 km data will sometimes have gaps\n",
    "\n",
    "hopefully after this interpolation, the gaps of invalid data are small enough to let gridfilling finish the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "from gridfill import fill\n",
    "\n",
    "masked_u = ma.masked_array(target_interped_u, invalid_interped)\n",
    "masked_v = ma.masked_array(target_interped_v, invalid_interped)\n",
    "\n",
    "kw = dict(eps=1e-4, relax=0.6, itermax=1e4, initzonal=False,\n",
    "          cyclic=False, verbose=True)\n",
    "\n",
    "# since data is 3d, use axes 2 and 1 since axis 0 is time\n",
    "filled_u, converged_u = fill(masked_u, 2, 1, **kw)\n",
    "filled_v, converged_v = fill(masked_v, 2, 1, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_filled = generate_mask(filled_u)\n",
    "print(f\"total invalid values on filled: {invalid_filled.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"percent invalid filled: {(invalid_interped.sum() - invalid_filled.sum()) / invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wow\n",
    "\n",
    "unsurprisingly gridfill did jack shit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formatting, saving, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-add coordinates, dimensions, and metadata to interpolated data\n",
    "darr_u = conv_to_dataarray(filled_u, target[\"xrds\"][\"u\"])\n",
    "darr_v = conv_to_dataarray(filled_v, target[\"xrds\"][\"v\"])\n",
    "target_interped_xrds = target[\"xrds\"].drop_vars([\"u\", \"v\"]).assign(u=darr_u, v=darr_v)\n",
    "target_interped_xrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = target[\"path\"].split(\".nc\")[0] + \"_interped.nc\"\n",
    "target_interped_xrds.to_netcdf(save_path)\n",
    "print(f\"saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display field to see if interpolation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_interp = xr_dataset_to_fieldset(target_interped_xrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[\"fs\"].U.show()\n",
    "fs_interp.U.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
