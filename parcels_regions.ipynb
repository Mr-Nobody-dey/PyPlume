{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parcels regions\n",
    "\n",
    "runs parcels on existing netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from parcels import FieldSet, ParticleSet\n",
    "from parcels import AdvectionRK4\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# it got annoying\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_6KM = 6\n",
    "DATA_2KM = 2\n",
    "DATA_1KM = 1\n",
    "\n",
    "filename_dict = {\n",
    "    DATA_6KM: \"west_coast_6km_hourly\",\n",
    "    DATA_2KM: \"west_coast_2km_hourly\",\n",
    "    DATA_1KM: \"west_coast_1km_hourly\"\n",
    "}\n",
    "\n",
    "def get_file_info(name, path, res):\n",
    "    xrds = xr.open_dataset(path)\n",
    "    # spherical mesh\n",
    "    fs = FieldSet.from_xarray_dataset(\n",
    "            xrds.copy(deep=True),\n",
    "            dict(U=\"u\",V=\"v\"),\n",
    "            dict(lat=\"lat\",lon=\"lon\",time=\"time\")\n",
    "        )\n",
    "    xrds.close()\n",
    "    lat = xrds[\"lat\"].values\n",
    "    lon = xrds[\"lon\"].values\n",
    "    return dict(\n",
    "        name = name,\n",
    "        path = path,\n",
    "        res = res,\n",
    "        xrds = xrds,\n",
    "        fs = fs,\n",
    "        timerng = (xrds[\"time\"].min().values, xrds[\"time\"].max().values),\n",
    "        timerng_secs = fs.gridset.dimrange(\"time\"),\n",
    "        lat = lat,\n",
    "        lon = lon,\n",
    "        domain = {\n",
    "            \"S\": lat.min(),\n",
    "            \"N\": lat.max(),\n",
    "            \"W\": lon.min(),\n",
    "            \"E\": lon.max(),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### netcdf files\n",
    "\n",
    "due to the lack of unique configurations for each file, there are limitations\n",
    "- must have same time domain\n",
    "- a domain that contains the defined spawn points later in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    get_file_info(\"tijuana_lin\", \"west_coast_1km_hourly_netcdfs/west_coast_1km_hourly_region_tijuana_river_lin.nc\", DATA_1KM),\n",
    "#     get_file_info(\"tijuana_invdist\", \"west_coast_1km_hourly_netcdfs/west_coast_1km_hourly_region_tijuana_river_invdist.nc\", DATA_1KM),\n",
    "    get_file_info(\"tijuana_simpterp\", \"west_coast_1km_hourly_netcdfs/west_coast_1km_hourly_region_tijuana_river_simpterp.nc\", DATA_1KM),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated gif stuff and particle simulation\n",
    "\n",
    "runs on each file you give it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation man very cool\n",
    "# reference tutorial_Agulhasparticles\n",
    "# needs ErrorCode for particle recovery\n",
    "from operator import attrgetter\n",
    "from parcels import ErrorCode, JITParticle, Variable\n",
    "\n",
    "max_v = 1.1 # for display purposes only, so the vector field colors don't change every iteration\n",
    "\n",
    "class TimedParticle(JITParticle):\n",
    "    lifetime = Variable(\"lifetime\", initial=0, dtype=np.float32)\n",
    "    spawntime = Variable(\"spawntime\", initial=attrgetter(\"time\"), dtype=np.float32)\n",
    "    \n",
    "def ParticleLifetime(particle, fieldset, time):\n",
    "    particle.lifetime += particle.dt\n",
    "\n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()\n",
    "    \n",
    "def exec_save_pset(data, i, runtime, dt, zpad = 3, save_snapshot = True, exec_pset = True):\n",
    "    \"\"\"\n",
    "    Saves a snapshot of a particle simulation and then executes.\n",
    "    \n",
    "    Args:\n",
    "        data (dict)\n",
    "        i (int)\n",
    "        runtime (float): seconds\n",
    "        dt (float): seconds\n",
    "    \"\"\"\n",
    "    if save_snapshot:\n",
    "        data[\"pset\"].show(savefile=str(data[\"snap_path\"])+\"/particles\"+str(i).zfill(zpad), field=\"vector\", vmax=max_v)\n",
    "    \n",
    "    if exec_pset:\n",
    "        # temporary - TODO make it only init once\n",
    "        k_plifetime = data[\"pset\"].Kernel(ParticleLifetime)\n",
    "\n",
    "        data[\"pset\"].execute(\n",
    "            AdvectionRK4 + k_plifetime,\n",
    "            runtime=timedelta(seconds=runtime),\n",
    "            dt=timedelta(seconds=dt),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle},\n",
    "            output_file=data[\"pfile\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up particle spawn points\n",
    "\n",
    "currently works where particles are generated at random positions with a bit of variations, and are released at set intervals.\n",
    "\n",
    "note about interpolation methods: only `linear` works if you want to use the FieldSet in a ParticleSet.\n",
    "\n",
    "TODO move simulation configurations somewhere else so its unique for every dataset\n",
    "\n",
    "move it all into json config shenanigans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_dt = timedelta(hours=4) # interval at which particles are released\n",
    "particles_per_dt = 10 # number of particles to release at each interval\n",
    "max_variation = 0.0015 # the max degrees a particle can be away from a spawn point\n",
    "\n",
    "# TODO: spawn points should probably be saved with a netcdf file\n",
    "spawn_points = np.array([\n",
    "#     (32.551707, -117.136)\n",
    "    (32.551707, -117.138),\n",
    "    (32.557, -117.138)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE same spawns for every dataset currently\n",
    "\n",
    "# path for saving particle trajectories and data\n",
    "part_path = Path(\"particledata\")\n",
    "part_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for f in files:\n",
    "    repetitions = math.floor(f[\"timerng_secs\"][1] / repeat_dt.total_seconds())\n",
    "    total = repetitions * particles_per_dt\n",
    "    lat_arr = np.zeros(total)\n",
    "    lon_arr = np.zeros(total)\n",
    "    time_arr = np.zeros(total)\n",
    "    for i in range(repetitions):\n",
    "        time_arr[particles_per_dt * i:particles_per_dt * (i + 1)] = repeat_dt.seconds * i\n",
    "\n",
    "    sp_lat = spawn_points.T[0][np.random.randint(0, len(spawn_points), total)]\n",
    "    sp_lon = spawn_points.T[1][np.random.randint(0, len(spawn_points), total)]\n",
    "    variances_lat = (np.random.random(total) * 2 - 1) * max_variation\n",
    "    variances_lon = (np.random.random(total) * 2 - 1) * max_variation\n",
    "\n",
    "    p_lats = sp_lat + variances_lat\n",
    "    p_lons = sp_lon + variances_lon\n",
    "\n",
    "    f[\"pset\"] = ParticleSet(fieldset=f[\"fs\"], pclass=TimedParticle, lon=p_lons, lat=p_lats, time=time_arr)\n",
    "    save_path = f\"{part_path}/particle_{f['name']}.nc\"\n",
    "    f[\"pfile\"] = f[\"pset\"].ParticleFile(save_path)\n",
    "    print(f\"Particle trajectories for {f['name']} will be saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f[\"pset\"].show(field=\"vector\", vmax=max_v) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting up particle execution\n",
    "\n",
    "there are a few things to configure before actually executing the simulation\n",
    "\n",
    "configure `snapshot_interval`, `p_dt`, `clear_directories`, and `save_snap` to your needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_interval = 3600 * 3 # seconds, determines how often to take a snapshot of the simulation\n",
    "p_dt = 300 # seconds, determines the dt of the simulation itself (does not impact snapshot interval)\n",
    "\n",
    "clear_directories = True # whether to clear the directory the snapshots will be saved in\n",
    "save_snap = True # whether to save snapshots of the simulation for gif generation later\n",
    "\n",
    "for f in files:\n",
    "    f[\"snap_num\"] = math.floor((f[\"timerng_secs\"][1] - f[\"timerng_secs\"][0]) / snapshot_interval)\n",
    "    f[\"last_int\"] = f[\"timerng_secs\"][1] - f[\"snap_num\"] * snapshot_interval\n",
    "    if f[\"last_int\"] == 0:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {f['snap_num'] + 1}\")\n",
    "    else:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {f['snap_num'] + 2}\")\n",
    "    f[\"snap_path\"] = Path(f\"snapshots_{filename_dict[f['res']]}/{f['name']}\")\n",
    "    f[\"snap_path\"].mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Path to save snapshots to: {f['snap_path']}\")\n",
    "    # only clear directory if desired or actually saving images\n",
    "    if clear_directories and save_snap:\n",
    "        for p in f[\"snap_path\"].glob(\"*.png\"):\n",
    "            p.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for f in files:\n",
    "    for i in range(f[\"snap_num\"]):\n",
    "        exec_save_pset(f, i, snapshot_interval, p_dt, save_snapshot=save_snap)\n",
    "\n",
    "    # save the second-to-last frame\n",
    "    exec_save_pset(f, f[\"snap_num\"], 0, 0, save_snapshot=save_snap, exec_pset=False)\n",
    "\n",
    "    # run the last interval (the remainder) if needed\n",
    "    if f[\"last_int\"] != 0:\n",
    "        exec_save_pset(f, f[\"snap_num\"] + 1, f[\"last_int\"], p_dt, save_snapshot=save_snap)\n",
    "        \n",
    "    f[\"pfile\"].export()\n",
    "\n",
    "print(\"all simulations done and snapshots saved (if simulation was saving snapshots)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gif generation\n",
    "\n",
    "don't have to run, requires [magick](https://imagemagick.org/index.php)\n",
    "\n",
    "the gifs will be saved `snapshots_west_coast_xkm_hourly/` where xkm is the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "gif_delay = 25 # ms\n",
    "\n",
    "if save_snap:\n",
    "    for f in files:\n",
    "        magick_sp = subprocess.Popen([\"magick\", \"-delay\", str(gif_delay), str(f[\"snap_path\"]) + \"/*.png\", f\"snapshots_{filename_dict[f['res']]}/partsim_{f['name']}.gif\"], \n",
    "                                       stdout=subprocess.PIPE,\n",
    "                                       stderr=subprocess.PIPE,\n",
    "                                       universal_newlines=True)\n",
    "        stdout, stderr = magick_sp.communicate()\n",
    "        print((stdout, stderr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
