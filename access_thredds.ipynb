{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# access thredds\n",
    "\n",
    "getting different regions of current data manually instead of using the auto-generated regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import utils\n",
    "from parcels_utils import xr_dataset_to_fieldset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url_6kmhourly = \"http://hfrnet-tds.ucsd.edu/thredds/dodsC/HFR/USWC/6km/hourly/RTV/HFRADAR_US_West_Coast_6km_Resolution_Hourly_RTV_best.ncd\"\n",
    "dataset_url_2kmhourly = \"http://hfrnet-tds.ucsd.edu/thredds/dodsC/HFR/USWC/2km/hourly/RTV/HFRADAR_US_West_Coast_2km_Resolution_Hourly_RTV_best.ncd\"\n",
    "dataset_url_1kmhourly = \"http://hfrnet-tds.ucsd.edu/thredds/dodsC/HFR/USWC/1km/hourly/RTV/HFRADAR_US_West_Coast_1km_Resolution_Hourly_RTV_best.ncd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunks = 50\n",
    "\n",
    "thredds_data = {\n",
    "    utils.DATA_6KM: xr.open_dataset(dataset_url_6kmhourly, chunks={\"time\": num_chunks}),\n",
    "    utils.DATA_2KM: xr.open_dataset(dataset_url_2kmhourly, chunks={\"time\": num_chunks}),\n",
    "    utils.DATA_1KM: xr.open_dataset(dataset_url_1kmhourly, chunks={\"time\": num_chunks})\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_region(data):\n",
    "    time_range = get_time_slice(data[2])\n",
    "    if data[5]:\n",
    "        lat_range = utils.expand_coord_rng(data[3], thredds_data[data[1]][\"lat\"].values)\n",
    "        lon_range = utils.expand_coord_rng(data[4], thredds_data[data[1]][\"lon\"].values)\n",
    "    else:\n",
    "        lat_range = data[3]\n",
    "        lon_range = data[4]\n",
    "    return dict(\n",
    "        name = data[0],\n",
    "        category = data[1],\n",
    "        time = time_range,\n",
    "        lat = lat_range,\n",
    "        lon = lon_range,\n",
    "        domain = {\n",
    "            \"S\": lat_range[0],\n",
    "            \"N\": lat_range[1],\n",
    "            \"W\": lon_range[0],\n",
    "            \"E\": lon_range[1],\n",
    "        }\n",
    "    )\n",
    "\n",
    "def get_latest_span(delta):\n",
    "    # GMT, data recorded hourly\n",
    "    time_now = np.datetime64(\"now\", \"h\")\n",
    "    return (time_now - delta, time_now)\n",
    "\n",
    "\n",
    "def get_time_slice(time_range):\n",
    "    if len(time_range) == 2:\n",
    "        return slice(np.datetime64(time_range[0]), np.datetime64(time_range[1]))\n",
    "    if len(time_range) == 3:\n",
    "        # step size is an integer in hours\n",
    "        return slice(np.datetime64(time_range[0]), np.datetime64(time_range[1]), time_range[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about tj_sample\n",
    "\n",
    "the purpose of tj_sample is a quick and dirty way to sample the thredds data from a bunch of different times to find out the positions of where data exists. data in close time ranges could all have the same holes in data, and we would never know if data was supposed to be there in the first place.\n",
    "\n",
    "so tj_sample is generated for the sole purpose of creating a mask showing where data shouldn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data = [\n",
    "    (\"tj_sample\", utils.DATA_1KM, (\"2020-01-10T00\", \"2020-08-11T00\", 200), (32.11093, 32.73124), (-117.565, -116.9924), False),\n",
    "    (\"tj_plume\", utils.DATA_1KM, (\"2020-08-01T00\", \"2020-08-14T14\"), (32.11093, 32.73124), (-117.565, -116.9924), False),\n",
    "    (\"tj_plume\", utils.DATA_2KM, (\"2020-08-01T00\", \"2020-08-14T14\"), (32.11093, 32.73124), (-117.565, -116.9924), True),\n",
    "    (\"tj_plume\", utils.DATA_6KM, (\"2020-08-01T00\", \"2020-08-14T14\"), (32.11093, 32.73124), (-117.565, -116.9924), True),\n",
    "#     (\"tijuana_river\", DATA_1KM, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.528, 32.71), (-117.29, -117.11), False),\n",
    "#     (\"tijuana_river\", DATA_2KM, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.524, 32.75), (-117.32, -117.09), False),\n",
    "#     (\"tijuana_river\", DATA_6KM, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.35, 32.80), (-117.33, -116.9), False),\n",
    "#     (\"tijuana_river_small\", DATA_1KM, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.528, 32.6), (-117.19, -117.11), False)\n",
    "#     (\"tijuana_river_now\", DATA_1KM, get_latest_span(np.timedelta64(300, \"D\")), (32.528, 32.71), (-117.29, -117.11), False),\n",
    "#     (\"tijuana_river_now\", DATA_2KM, get_latest_span(np.timedelta64(300, \"D\")), (32.524, 32.75), (-117.32, -117.09), False),\n",
    "#     (\"tijuana_river_now\", DATA_6KM, (\"2019-09-28T21:00\", \"2020-07-24T20\"), (32.35, 32.80), (-117.33, -116.9), False),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = []\n",
    "for rd in region_data:\n",
    "    new_reg = get_region(rd)\n",
    "    new_reg[\"dataset\"] = thredds_data[new_reg[\"category\"]].sel(\n",
    "        time=new_reg[\"time\"],\n",
    "        lat=slice(new_reg[\"lat\"][0], new_reg[\"lat\"][1]),\n",
    "        lon=slice(new_reg[\"lon\"][0], new_reg[\"lon\"][1]),\n",
    "    )\n",
    "    regions.append(new_reg)\n",
    "    print(f\"region {new_reg['name']} data megabytes: {new_reg['dataset'].nbytes / 1024 / 1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in regions:\n",
    "    save_dir = utils.create_path(utils.CURRENT_NETCDF_DIR / utils.filename_dict[r[\"category\"]])\n",
    "    filename = f\"{r['name']}.nc\"\n",
    "    # save file\n",
    "    r[\"dataset\"].to_netcdf(save_dir / filename)\n",
    "    print(f\"saved to {save_dir / filename}\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
