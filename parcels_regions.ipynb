{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parcels regions\n",
    "\n",
    "runs parcels on existing netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from parcels import FieldSet, ParticleSet\n",
    "from parcels import AdvectionRK4\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import utils\n",
    "from utils import get_file_info, load_config\n",
    "\n",
    "# ignore annoying deprecation warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import cartopy\n",
    "\n",
    "# ignore divide by nan error that happens constantly with parcels\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration\n",
    "\n",
    "change the contents of `configs` for the simulation configuration you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    \"parcels_configs/tijuana_lin_cfg.json\",\n",
    "    \"parcels_configs/tijuana_simpterp_cfg.json\",\n",
    "    \"parcels_configs/tijuana_lin_aggr_cfg.json\"\n",
    "]\n",
    "\n",
    "loaded_configs = [load_config(path) for path in configs]\n",
    "files = [get_file_info(cfg[\"name\"], cfg[\"netcdf_path\"], cfg[\"resolution\"], cfg[\"parcels_config\"]) for cfg in loaded_configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated gif stuff and particle simulation\n",
    "\n",
    "runs on each file you give it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation man very cool\n",
    "# reference tutorial_Agulhasparticles\n",
    "# needs ErrorCode for particle recovery\n",
    "from operator import attrgetter\n",
    "from parcels import ErrorCode, JITParticle, Variable\n",
    "\n",
    "max_v = 1.1 # for display purposes only, so the vector field colors don't change every iteration\n",
    "\n",
    "class TimedParticle(JITParticle):\n",
    "    lifetime = Variable(\"lifetime\", initial=0, dtype=np.float32)\n",
    "    spawntime = Variable(\"spawntime\", initial=attrgetter(\"time\"), dtype=np.float32)\n",
    "    \n",
    "def ParticleLifetime(particle, fieldset, time):\n",
    "    particle.lifetime += particle.dt\n",
    "\n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()\n",
    "    \n",
    "def exec_save_pset(data, i, runtime, dt, zpad=3, save_snapshot=True, exec_pset=True):\n",
    "    \"\"\"\n",
    "    Saves a snapshot of a particle simulation and then executes.\n",
    "    \n",
    "    Args:\n",
    "        data (dict)\n",
    "        i (int)\n",
    "        runtime (float): seconds\n",
    "        dt (float): seconds\n",
    "    \"\"\"\n",
    "    if save_snapshot:\n",
    "        data[\"pset\"].show(savefile=str(data[\"snap_path\"])+\"/particles\"+str(i).zfill(zpad), field=\"vector\", vmax=max_v)\n",
    "    \n",
    "    if exec_pset:\n",
    "        # temporary - TODO make it only init once\n",
    "        k_plifetime = data[\"pset\"].Kernel(ParticleLifetime)\n",
    "\n",
    "        data[\"pset\"].execute(\n",
    "            AdvectionRK4 + k_plifetime,\n",
    "            runtime=timedelta(seconds=runtime),\n",
    "            dt=timedelta(seconds=dt),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle},\n",
    "            output_file=data[\"pfile\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParticleSet and spawn point setup\n",
    "\n",
    "note about interpolation methods: only `linear` works if you want to use the FieldSet in a ParticleSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "part_path = Path(\"particledata\")\n",
    "part_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    repeat_dt = timedelta(seconds=cfg[\"repeat_dt\"]) # interval at which particles are released\n",
    "    cfg[\"spawn_points\"] = np.array(cfg[\"spawn_points\"]) # particle spawns will be randomly chosen between these points\n",
    "    \n",
    "    repetitions = math.floor(f[\"timerng_secs\"][1] / repeat_dt.total_seconds())\n",
    "    # the total number of particles that will exist in the simulation\n",
    "    total = repetitions * cfg[\"particles_per_dt\"]\n",
    "    lat_arr = np.zeros(total)\n",
    "    lon_arr = np.zeros(total)\n",
    "    time_arr = np.zeros(total)\n",
    "    for i in range(repetitions):\n",
    "        time_arr[cfg[\"particles_per_dt\"] * i:cfg[\"particles_per_dt\"] * (i + 1)] = repeat_dt.total_seconds() * i\n",
    "\n",
    "    # randomly select spawn points from the given config\n",
    "    sp_lat = cfg[\"spawn_points\"].T[0][np.random.randint(0, len(cfg[\"spawn_points\"]), total)]\n",
    "    sp_lon = cfg[\"spawn_points\"].T[1][np.random.randint(0, len(cfg[\"spawn_points\"]), total)]\n",
    "    # vary spawn locations using max_variation\n",
    "    variances_lat = (np.random.random(total) * 2 - 1) * cfg[\"max_variation\"]\n",
    "    variances_lon = (np.random.random(total) * 2 - 1) * cfg[\"max_variation\"]\n",
    "\n",
    "    p_lats = sp_lat + variances_lat\n",
    "    p_lons = sp_lon + variances_lon\n",
    "\n",
    "    # set up ParticleSet and ParticleFile\n",
    "    f[\"pset\"] = ParticleSet(fieldset=f[\"fs\"], pclass=TimedParticle, lon=p_lons, lat=p_lats, time=time_arr)\n",
    "    save_path = f\"{part_path}/particle_{f['name']}.nc\"\n",
    "    f[\"pfile\"] = f[\"pset\"].ParticleFile(save_path)\n",
    "    print(f\"Particle trajectories for {f['name']} will be saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f[\"pset\"].show(field=\"vector\", vmax=max_v) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulation setup and execution\n",
    "\n",
    "simulation parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting up times, intervals, and paths for simulation\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    cfg[\"snap_num\"] = math.floor((f[\"timerng_secs\"][1] - f[\"timerng_secs\"][0]) / cfg[\"snapshot_interval\"])\n",
    "    cfg[\"last_int\"] = f[\"timerng_secs\"][1] - cfg[\"snap_num\"] * cfg[\"snapshot_interval\"]\n",
    "    if cfg[\"last_int\"] == 0:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {cfg['snap_num'] + 1}\")\n",
    "    else:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {cfg['snap_num'] + 2}\")\n",
    "    f[\"snap_path\"] = Path(f\"snapshots_{utils.filename_dict[f['res']]}/{f['name']}\")\n",
    "    f[\"snap_path\"].mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Path to save snapshots to: {f['snap_path']}\")\n",
    "    # only clear directory if desired or actually saving images\n",
    "    if cfg[\"save_snapshots\"]:\n",
    "        for p in f[\"snap_path\"].glob(\"*.png\"):\n",
    "            p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execution of all simulation configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# execution of simulation\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    for i in range(cfg[\"snap_num\"]):\n",
    "        exec_save_pset(f, i, cfg[\"snapshot_interval\"], cfg[\"simulation_dt\"], save_snapshot=cfg[\"save_snapshots\"])\n",
    "\n",
    "    # save the second-to-last frame\n",
    "    exec_save_pset(f, cfg[\"snap_num\"], 0, 0, save_snapshot=cfg[\"save_snapshots\"], exec_pset=False)\n",
    "\n",
    "    # run the last interval (the remainder) if needed\n",
    "    if cfg[\"last_int\"] != 0:\n",
    "        exec_save_pset(f, cfg[\"snap_num\"] + 1, cfg[\"last_int\"], cfg[\"simulation_dt\"], save_snapshot=cfg[\"save_snapshots\"])\n",
    "        \n",
    "    f[\"pfile\"].export()\n",
    "    f[\"pfile\"].close()\n",
    "\n",
    "print(\"all simulations done and snapshots saved (if simulation was saving snapshots)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gif generation\n",
    "\n",
    "don't have to run, requires [magick](https://imagemagick.org/index.php)\n",
    "\n",
    "the gifs will be saved `snapshots_west_coast_xkm_hourly/` where xkm is the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "gif_delay = 25 # ms\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    if f[\"cfg\"][\"save_snapshots\"]:\n",
    "        magick_sp = subprocess.Popen(\n",
    "            [\n",
    "                \"magick\", \"-delay\", str(gif_delay),\n",
    "                str(f[\"snap_path\"]) + \"/*.png\", # path to the snapshots to stitch\n",
    "                f\"snapshots_{utils.filename_dict[f['res']]}/partsim_{f['name']}.gif\" # path to save gif to\n",
    "            ],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        stdout, stderr = magick_sp.communicate()\n",
    "        print((stdout, stderr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
