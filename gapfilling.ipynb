{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing data interpolation\n",
    "\n",
    "statistics is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential shenanigans\n",
    "\n",
    "\"Several techniques have been used to fill the gaps in either the UWLS or OI derived total vector maps.\n",
    "\n",
    "These are implemented using covariance derived from normal mode analysis (Lipphardt et al. 2000), open-boundary modal analysis (OMA) (Kaplan and Lekien 2007), and empirical orthogonal function (EOF) analysis (Beckers and Rixen 2003; Alvera-Azc√°rate et al. 2005); and using idealized or smoothed observed covariance (Davis 1985).\"\n",
    "\n",
    "- normal mode analysis\n",
    "- open-boundary modal analysis (OMA)\n",
    "- empirical orthogonal function analysis (EOF)\n",
    "- use idealized/smoothed observed covariance\n",
    "\n",
    "---\n",
    "\n",
    "### other ideas\n",
    "\n",
    "DINEOF (could only find an implementation in R)\n",
    "\n",
    "to be honest I don't understand any of these methods but they look cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### currently implemented:\n",
    "\n",
    "rip data straight from the lower resolution data for areas where data is considered missing in the high resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from parcels import FieldSet\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "from utils import generate_mask, conv_to_dataarray\n",
    "from parcels_utils import get_file_info, xr_dataset_to_fieldset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target and interp_references\n",
    "\n",
    "`target` is the data you are interpolating.\n",
    "\n",
    "`interp_references` is a list of reference data to interpolate from. A few specifications:\n",
    "- should be ordered from most accurate data to least accurate\n",
    "- time domain should be identical or bigger than the one of the target\n",
    "- lat and lon domain should be bigger than the target's to prevent any out-of-bounds complications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = get_file_info(\"current_netcdfs/west_coast_1km_hourly/tijuana_river_now.nc\", 1, name=\"target\")\n",
    "\n",
    "interp_references = [\n",
    "    get_file_info(\"current_netcdfs/west_coast_2km_hourly/region0.nc\", 2, name=\"ref2km\"),\n",
    "    get_file_info(\"current_netcdfs/west_coast_6km_hourly/region0.nc\", 6, name=\"ref6km\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolation type\n",
    "\n",
    "more information can be found in the `tutorial_interpolation` notebook\n",
    "\n",
    "EDIT: just use `linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_interp_method = \"linear\"\n",
    "\n",
    "for r in interp_references:\n",
    "    r[\"fs_flat\"].U.interp_method = reference_interp_method\n",
    "    r[\"fs_flat\"].V.interp_method = reference_interp_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan values and parcels\n",
    "\n",
    "note that when this xarray Dataset is passed into parcels, all the nan values change to 0 and the mask generation won't work anymore\n",
    "\n",
    "so the Dataset is copied for use with the FieldSet instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid = generate_mask(target[\"xrds\"][\"u\"].values)\n",
    "num_invalid = invalid.sum()\n",
    "print(f\"total invalid values on target data: {num_invalid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_pos = np.where(invalid)\n",
    "invalid_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use of Parcels Field for interpolation\n",
    "\n",
    "indexing Field values goes [time, depth, lat, lon]\n",
    "\n",
    "Field does interpolation automatically when indexing values between it's coordinate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note\n",
    "\n",
    "https://stackoverflow.com/questions/12923586/nearest-neighbor-search-python\n",
    "\n",
    "in theory, the latitude and longitude values are equally spaced. however, the difference between the coordinate values always flucuates a very small amount between two distinct values, so it's not perfectly equally spaced.\n",
    "\n",
    "from testing, this causes enough error to completely change a simulation, so a kdtree must be used.\n",
    "\n",
    "# another note\n",
    "\n",
    "I just looked at the particle trajectory that used the indexing without kd trees and it looked MAJORLY fucked up, so I probably wrote something wrong with the method. TODO will come back to this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial\n",
    "\n",
    "# set up kdtrees for all references\n",
    "for f in interp_references:\n",
    "    f[\"latkdtree\"] = scipy.spatial.cKDTree(np.array([f[\"lat\"]]).T)\n",
    "    f[\"lonkdtree\"] = scipy.spatial.cKDTree(np.array([f[\"lon\"]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nearest_current(ref, t, lat, lon):\n",
    "    index = (t, ref[\"latkdtree\"].query([lat])[1], ref[\"lonkdtree\"].query([lon])[1])\n",
    "    return ref[\"xrds\"][\"u\"].values[index], ref[\"xrds\"][\"v\"].values[index]\n",
    "\n",
    "\n",
    "def get_current_interp(ref, t, lat, lon):\n",
    "    return ref[\"fs_flat\"].U[t, 0, lat, lon], ref[\"fs_flat\"].V[t, 0, lat, lon]\n",
    "\n",
    "\n",
    "def get_interped(i, ref, invalid_where):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        i (int): index on invalid_where\n",
    "        ref (dict): reference Dataset\n",
    "        invalid_where (array-like): (3, n) dimensional array representing all invalid positions\n",
    "    \n",
    "    Returns:\n",
    "        (u, v): (nan, nan) if no data was found, interpolated values otherwise\n",
    "    \"\"\"\n",
    "    time_diff = np.diff(ref[\"fs_flat\"].U.grid.time)[0]\n",
    "    t = invalid_where[0][i]\n",
    "    lat = target[\"lat\"][invalid_where[1][i]]\n",
    "    lon = target[\"lon\"][invalid_where[2][i]]\n",
    "    current_u, current_v = get_current_interp(ref, t * time_diff, lat, lon)\n",
    "    current = current_u + current_v\n",
    "    # TODO: testing whether `current == 0` actually helps or not\n",
    "    # basically makes interpolation a bit more \"aggressive\"?\n",
    "    if np.isnan(get_nearest_current(ref, t, lat, lon)[0]):\n",
    "        return np.nan, np.nan\n",
    "    return current_u, current_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### today i learned\n",
    "\n",
    "doing DataArray.values does not return a numpy array copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_st = time.time()\n",
    "target_interped_u = target[\"xrds\"][\"u\"].values.copy()\n",
    "target_interped_v = target[\"xrds\"][\"v\"].values.copy()\n",
    "invalid_interped = invalid.copy()\n",
    "for f in interp_references:\n",
    "    invalid_pos_new = np.where(invalid_interped)\n",
    "    num_invalid_new = invalid_interped.sum()\n",
    "    arr_u = np.zeros(num_invalid_new)\n",
    "    arr_v = np.zeros(num_invalid_new)\n",
    "    for i in range(num_invalid_new):\n",
    "        c_u, c_v = get_interped(i, f, invalid_pos_new)\n",
    "        arr_u[i] = c_u\n",
    "        arr_v[i] = c_v\n",
    "    target_interped_u[invalid_pos_new] = arr_u\n",
    "    target_interped_v[invalid_pos_new] = arr_v\n",
    "    invalid_interped = generate_mask(target_interped_u)\n",
    "    print(f\"total invalid values after interpolation with {f['name']}: {invalid_interped.sum()}\")\n",
    "    print(f\"    values filled: {num_invalid_new - invalid_interped.sum()}\")\n",
    "time_en = time.time()\n",
    "print(f\"time elapsed: {time_en - time_st}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result: interpolate only when interpolated current is non-zero OR not nan\n",
    "\n",
    "total invalid values after interpolation with ref2km: 1562\n",
    "\n",
    "    values filled: 12589\n",
    "total invalid values after interpolation with ref6km: 2\n",
    "\n",
    "    values filled: 1560\n",
    "    \n",
    "YEP it fills a lot more values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result: interpolate whenever nearest data point is not nan\n",
    "\n",
    "total invalid values after interpolation with ref2km: 3176\n",
    "\n",
    "    values filled: 10975\n",
    "total invalid values after interpolation with ref6km: 758\n",
    "\n",
    "    values filled: 2418"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total invalid values on interpolated: {invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gridfill invalid values after interpolation\n",
    "\n",
    "in theory there should still be invalid spaces left over because the 6 km data will sometimes have gaps\n",
    "\n",
    "hopefully after this interpolation, the gaps of invalid data are small enough to let gridfilling finish the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "from gridfill import fill\n",
    "\n",
    "masked_u = ma.masked_array(target_interped_u, invalid_interped)\n",
    "masked_v = ma.masked_array(target_interped_v, invalid_interped)\n",
    "\n",
    "kw = dict(eps=1e-4, relax=0.6, itermax=1e4, initzonal=False,\n",
    "          cyclic=False, verbose=True)\n",
    "\n",
    "# since data is 3d, use axes 2 and 1 since axis 0 is time\n",
    "filled_u, converged_u = fill(masked_u, 2, 1, **kw)\n",
    "filled_v, converged_v = fill(masked_v, 2, 1, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_filled = generate_mask(filled_u)\n",
    "print(f\"total invalid values on filled: {invalid_filled.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"percent invalid filled: {(invalid_interped.sum() - invalid_filled.sum()) / invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wow\n",
    "\n",
    "unsurprisingly gridfill did jack shit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formatting, saving, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-add coordinates, dimensions, and metadata to interpolated data\n",
    "darr_u = conv_to_dataarray(filled_u, target[\"xrds\"][\"u\"])\n",
    "darr_v = conv_to_dataarray(filled_v, target[\"xrds\"][\"v\"])\n",
    "target_interped_xrds = target[\"xrds\"].drop_vars([\"u\", \"v\"]).assign(u=darr_u, v=darr_v)\n",
    "target_interped_xrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = target[\"path\"].split(\".nc\")[0] + \"_interped.nc\"\n",
    "target_interped_xrds.to_netcdf(save_path)\n",
    "print(f\"saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display field to see if interpolation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_interp = xr_dataset_to_fieldset(target_interped_xrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[\"fs\"].U.show()\n",
    "fs_interp.U.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
