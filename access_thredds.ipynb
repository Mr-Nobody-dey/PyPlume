{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# access thredds\n",
    "\n",
    "Download sliced netcdf data from Thredds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import src.utils as utils\n",
    "from src.thredds_utils import ThreddsCode, slice_dataset, retrieve_dataset\n",
    "from src.parcels_utils import xr_dataset_to_fieldset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_span(delta):\n",
    "    # GMT, data recorded hourly\n",
    "    time_now = np.datetime64(\"now\", \"h\")\n",
    "    return (time_now - delta, time_now)\n",
    "\n",
    "\n",
    "def get_time_slice(time_range):\n",
    "    if len(time_range) == 2:\n",
    "        return slice(np.datetime64(time_range[0]), np.datetime64(time_range[1]))\n",
    "    if len(time_range) == 3:\n",
    "        # step size is an integer in hours\n",
    "        return slice(np.datetime64(time_range[0]), np.datetime64(time_range[1]), time_range[2])\n",
    "    \n",
    "\n",
    "def get_regs_year(year, name, lat_rng, lon_rng):\n",
    "    regions = []\n",
    "    months = np.arange(str(year), str(year + 1), dtype=\"datetime64[M]\")\n",
    "    for m in months:\n",
    "        days = np.arange(m, m + np.timedelta64(1, \"M\"), dtype=\"datetime64[D]\")\n",
    "        timerng = (np.datetime64(days[0], \"h\"), days[-1] + np.timedelta64(23, \"h\"))\n",
    "        for code in (ThreddsCode.USWC_1KM_HOURLY, ThreddsCode.USWC_2KM_HOURLY, ThreddsCode.USWC_6KM_HOURLY):\n",
    "            regions.append({\n",
    "                \"name\": f\"{name}_{m}\",\n",
    "                \"code\": code,\n",
    "                \"time_range\": timerng,\n",
    "                \"lat_range\": lat_rng,\n",
    "                \"lon_range\": lon_rng,\n",
    "                \"include_range_endpoints\": False if code == ThreddsCode.USWC_1KM_HOURLY else True\n",
    "            })\n",
    "    return regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### format of region_data stuff\n",
    "\n",
    "(name, resolution, time range, lat range, lon range, expand range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### about tj_sample\n",
    "\n",
    "the purpose of tj_sample is a quick and dirty way to sample the thredds data from a bunch of different times to find out the positions of where data exists. data in close time ranges could all have the same holes in data, and we would never know if data was supposed to be there in the first place.\n",
    "\n",
    "so tj_sample is generated for the sole purpose of creating a mask showing where data shouldn't exist.\n",
    "\n",
    "## data masks\n",
    "\n",
    "where is there data? every timestep of HFR data is not always complete, so we need to know what nan points were supposed to have data and which ones were never meant to have data.\n",
    "\n",
    "A good way to find this out is to take several slices of data over a long period of time and check the coverage of each timestamp. This is the easiest way to kind of see the true coverage of HFR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple reference\n",
    "# (name, region code, time range, lat range, lon range, include domain endpoints)\n",
    "region_data = [\n",
    "#     {\n",
    "#         \"name\": \"tj_sample\",\n",
    "#         \"code\": ThreddsCode.USWC_1KM_HOURLY,\n",
    "#         \"time_range\": (\"2019-01-01T00\", \"2021-01-01T00\", 300),\n",
    "#         \"lat_range\": (32.11093, 32.73124),\n",
    "#         \"lon_range\": (-117.565, -116.9924),\n",
    "#         \"include_range_endpoints\": False\n",
    "#     },\n",
    "    # {\n",
    "    #     \"name\": \"hunington_latest\",\n",
    "    #     \"code\": ThreddsCode.USWC_6KM_HOURLY,\n",
    "    #     \"time_range\": (\"2021-10-01T00\", \"2021-10-10T00\"),\n",
    "    #     \"lat_range\": (32, 34.2),\n",
    "    #     \"lon_range\": (-119, -117.4),\n",
    "    #     \"include_range_endpoints\": True\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"tj_plume_1km_2020-03\",\n",
    "        \"code\": ThreddsCode.USWC_1KM_HOURLY,\n",
    "        \"time_range\": (\"2020-03-09T01:00\", \"2020-03-14T01:00\"),\n",
    "        \"lat_range\": (32.11093, 32.73124),\n",
    "        \"lon_range\": (-117.565, -116.9924),\n",
    "        \"include_range_endpoints\": True,\n",
    "        \"generate_mask\": True\n",
    "    },\n",
    "    # {\n",
    "    #   \"name\": \"hycom_mwbproj\",\n",
    "    #   \"code\": ThreddsCode.DATA_HYCOMFORE,\n",
    "    #   \"time_range\": (np.datetime64(\"now\") - np.timedelta64(1, \"D\"), np.datetime64(\"now\") + np.timedelta64(7, \"D\")),\n",
    "    #   \"lat_range\": (22, 45),\n",
    "    #   \"lon_range\": (273, 295),\n",
    "    #   \"include_range_endpoints\": False\n",
    "    # },\n",
    "    # stuff below here hasn't been updated with the new format\n",
    "#     (\"tj_plume\", ThreddsCode.USWC_1KM_HOURLY, (\"2020-08-01T01\", \"2020-08-14T13\"), (32.11093, 32.73124), (-117.565, -116.9924), False),\n",
    "#     (\"tj_plume\", ThreddsCode.USWC_2KM_HOURLY, (\"2020-08-01T01\", \"2020-08-14T13\"), (32.11093, 32.73124), (-117.565, -116.9924), True),\n",
    "#     (\"tj_plume\", ThreddsCode.USWC_6KM_HOURLY, (\"2020-08-01T01\", \"2020-08-14T13\"), (32.11093, 32.73124), (-117.565, -116.9924), True),\n",
    "#     (\"tijuana_river\", ThreddsCode.USWC_1KM_HOURLY, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.528, 32.71), (-117.29, -117.11), False),\n",
    "#     (\"tijuana_river\", ThreddsCode.USWC_2KM_HOURLY, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.524, 32.75), (-117.32, -117.09), False),\n",
    "#     (\"tijuana_river\", ThreddsCode.USWC_6KM_HOURLY, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.35, 32.80), (-117.33, -116.9), False),\n",
    "#     (\"tijuana_river_small\", ThreddsCode.USWC_1KM_HOURLY, (\"2020-06-16T21\", \"2020-06-23T21\"), (32.528, 32.6), (-117.19, -117.11), False)\n",
    "#     (\"tijuana_river_now\", ThreddsCode.USWC_1KM_HOURLY, get_latest_span(np.timedelta64(300, \"D\")), (32.528, 32.71), (-117.29, -117.11), False),\n",
    "#     (\"tijuana_river_now\", ThreddsCode.USWC_2KM_HOURLY, get_latest_span(np.timedelta64(300, \"D\")), (32.524, 32.75), (-117.32, -117.09), False),\n",
    "#     (\"tijuana_river_now\", ThreddsCode.USWC_6KM_HOURLY, (\"2019-09-28T21:00\", \"2020-07-24T20\"), (32.35, 32.80), (-117.33, -116.9), False),\n",
    "#     (\"missing_buoy\", ThreddsCode.USWC_6KM_HOURLY, (\"2021-01-29T05\", \"2021-02-02T16\"), (33.15, 33.778072), (-118.697986, -117.6), False)\n",
    "]\n",
    "\n",
    "\n",
    "# for rd in get_regs_year(2020, \"tj_plume\", (32.11093, 32.73124), (-117.565, -116.9924)):\n",
    "#     region_data.append(rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions = []\n",
    "for rd in region_data:\n",
    "    dataset = slice_dataset(\n",
    "        rd[\"code\"], rd[\"time_range\"], rd[\"lat_range\"],\n",
    "        rd[\"lon_range\"], inclusive=rd[\"include_range_endpoints\"]\n",
    "    )\n",
    "    new_reg = {\"name\": rd[\"name\"], \"dataset\": dataset}\n",
    "    regions.append(new_reg)\n",
    "    print(f\"region {new_reg['name']} data megabytes: {new_reg['dataset'].nbytes / 1024 / 1024}\")\n",
    "    if rd.get(\"generate_mask\", False):\n",
    "        # automatically generate a mask\n",
    "        full = retrieve_dataset(rd[\"code\"])\n",
    "        hours = (full[\"time\"].max() - full[\"time\"].min()) / np.timedelta64(1, \"h\")\n",
    "        # 50 equal timesteps, should be a good enough sample to show coverage right?\n",
    "        step = int(hours / 50)\n",
    "        mask = slice_dataset(\n",
    "            rd[\"code\"], time_range=(full[\"time\"].min().values, full[\"time\"].max().values, step),\n",
    "            lat_range=rd[\"lat_range\"], lon_range=rd[\"lon_range\"],\n",
    "            inclusive=rd[\"include_range_endpoints\"]\n",
    "        )\n",
    "        new_reg = {\"name\": f\"{rd['name']}_mask\", \"dataset\": mask}\n",
    "        regions.append(new_reg)\n",
    "        print(f\"region {new_reg['name']} data megabytes: {new_reg['dataset'].nbytes / 1024 / 1024}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, r in enumerate(regions):\n",
    "    save_dir = utils.create_path(utils.CURRENT_NETCDF_DIR)\n",
    "    filename = f\"{r['name']}.nc\"\n",
    "    # save file\n",
    "    r[\"dataset\"].to_netcdf(save_dir / filename)\n",
    "    print(f\"saved to {save_dir / filename}\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0eba6c4b74a2f8ee4f11000f0a8df2ef42b87fd4ae75b12d527f1b8c08aad6c6"
  },
  "kernelspec": {
   "display_name": "py3-parcels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
