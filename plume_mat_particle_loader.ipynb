{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load mat pts files into a parcels trajectory output netcdf\n",
    "\n",
    "note that the mat pts files have a fuckton of rows that are just all nan, so the data is pretty bloated.\n",
    "\n",
    "TODO: dont save the completely nan rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import xarray as xr\n",
    "\n",
    "import utils\n",
    "from parcels_utils import arrays_to_particlefilenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(time):\n",
    "    d = \"\".join(str(time).split(\"T\")[0].split(\"-\"))\n",
    "    t = \"\".join(str(time).split(\"T\")[1].split(\":\")[:2])\n",
    "    return d, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = np.datetime64(\"2020-08-10T00:30\")\n",
    "end_time = np.datetime64(\"2020-08-10T23:30\")\n",
    "incr = np.timedelta64(1, \"h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = None\n",
    "current_time = start_time\n",
    "\n",
    "while current_time <= end_time:\n",
    "    date_parsed, time_parsed = parse_time(current_time)\n",
    "    filename = f\"pts_{date_parsed}_{time_parsed}.mat\"\n",
    "    if (utils.MATLAB_DIR / filename).is_file():\n",
    "        pts_mat = scipy.io.loadmat(utils.MATLAB_DIR / filename)\n",
    "    else:\n",
    "        print(f\"{utils.MATLAB_DIR / filename} not found\")\n",
    "        current_time += incr\n",
    "        continue\n",
    "    lon = np.array(pts_mat[\"xf\"], dtype=np.float32)\n",
    "    lat = np.array(pts_mat[\"yf\"], dtype=np.float32)\n",
    "    time = np.full((lon.size, 1), current_time)\n",
    "    ds_curr = arrays_to_particlefilenc(time, lat, lon)\n",
    "    if dataset is None:\n",
    "        dataset = ds_curr\n",
    "    else:\n",
    "        dataset = xr.concat([dataset, ds_curr], dim=\"obs\")\n",
    "    current_time += incr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invalid rows are the rows where particles randomly disappear or reappear in the middle of the simulation\n",
    "\n",
    "since I have no idea how the plume tracker code handles particle generation I have no idea how to interpret this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "invalid_rows = []\n",
    "for i in range(dataset.dims[\"traj\"]):\n",
    "    num_real = (~np.isnan(dataset[\"lat\"][i].values)).sum()\n",
    "    if np.isnan(dataset[\"lat\"][i, :num_real]).any():\n",
    "        invalid_rows.append(i)\n",
    "invalid_rows = np.array(invalid_rows)\n",
    "invalid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, time = parse_time(start_time)\n",
    "dataset.to_netcdf(utils.PARTICLE_NETCDF_DIR / f\"plume_pts_{date}_{time}.nc\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
