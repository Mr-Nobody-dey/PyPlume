{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parcels regions\n",
    "\n",
    "runs parcels on existing netcdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from parcels import FieldSet, ParticleSet\n",
    "from datetime import timedelta\n",
    "\n",
    "import utils\n",
    "from utils import load_config, create_path, add_noise\n",
    "from parcels_utils import get_file_info\n",
    "from plot_utils import plot_particles_age\n",
    "\n",
    "# ignore annoying deprecation warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "import cartopy\n",
    "\n",
    "# ignore divide by nan error that happens constantly with parcels\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration\n",
    "\n",
    "change the contents of `configs` for the simulation configuration you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    \"parcels_configs/tijuana_interped.json\",\n",
    "#     \"parcels_configs/tijuana_lin.json\",\n",
    "#     \"parcels_configs/tijuana_lin_aggr.json\",\n",
    "#     \"parcels_configs/tijuana_less.json\",\n",
    "#     \"parcels_configs/tijuana_now.json\",\n",
    "#     \"parcels_configs/tijuana_onerep.json\",\n",
    "#     \"parcels_configs/tijuana_range.json\",\n",
    "]\n",
    "\n",
    "loaded_configs = [load_config(path) for path in configs]\n",
    "files = [get_file_info(cfg[\"netcdf_path\"], cfg[\"resolution\"], name=cfg[\"name\"], parcels_cfg=cfg[\"parcels_config\"]) for cfg in loaded_configs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animated gif stuff and particle simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animation man very cool\n",
    "# reference tutorial_Agulhasparticles\n",
    "# needs ErrorCode for particle recovery\n",
    "from operator import attrgetter\n",
    "from parcels import ErrorCode, JITParticle, Variable, AdvectionRK4\n",
    "\n",
    "max_v = 0.6 # for display purposes only, so the vector field colors don't change every iteration\n",
    "\n",
    "class TimedParticle(JITParticle):\n",
    "    lifetime = Variable(\"lifetime\", initial=0, dtype=np.float32)\n",
    "    spawntime = Variable(\"spawntime\", initial=attrgetter(\"time\"), dtype=np.float32)\n",
    "\n",
    "    \n",
    "def ParticleLifetime(particle, fieldset, time):\n",
    "    particle.lifetime += particle.dt\n",
    "\n",
    "    \n",
    "def DeleteParticle(particle, fieldset, time):\n",
    "    particle.delete()\n",
    "\n",
    "    \n",
    "def exec_save_pset(data, i, runtime, dt, zpad=3, ages=True, save_snapshot=True, exec_pset=True):\n",
    "    \"\"\"\n",
    "    Saves a snapshot of a particle simulation and then executes.\n",
    "    \n",
    "    Args:\n",
    "        data (dict)\n",
    "        i (int)\n",
    "        runtime (float): seconds\n",
    "        dt (float): seconds\n",
    "    \"\"\"\n",
    "    if len(data[\"pset\"]) == 0:\n",
    "        print(\"ParticleSet is empty. Passing...\", file=sys.stderr)\n",
    "        return\n",
    "    \n",
    "    if save_snapshot:\n",
    "        days = (data[\"timerng\"][1] - data[\"timerng\"][0]) / np.timedelta64(1, 'D')\n",
    "        field = None if ages else \"vector\"\n",
    "        if \"shown_domain\" in data[\"cfg\"]:\n",
    "            dom = data[\"cfg\"][\"shown_domain\"]\n",
    "        else:\n",
    "            dom = data[\"domain\"]\n",
    "        plot_particles_age(data[\"pset\"], dom, field=field,\n",
    "                           savefile=str(data[\"snap_path\"])+\"/particles\"+str(i).zfill(zpad),\n",
    "                           vmax=days, field_vmax=max_v)\n",
    "    \n",
    "    if exec_pset:\n",
    "        # temporary - TODO make it only init once\n",
    "        k_plifetime = data[\"pset\"].Kernel(ParticleLifetime)\n",
    "\n",
    "        data[\"pset\"].execute(\n",
    "            AdvectionRK4 + k_plifetime,\n",
    "            runtime=timedelta(seconds=runtime),\n",
    "            dt=timedelta(seconds=dt),\n",
    "            recovery={ErrorCode.ErrorOutOfBounds: DeleteParticle},\n",
    "            output_file=data[\"pfile\"]\n",
    "        )\n",
    "        \n",
    "        \n",
    "def parse_time_range(time_range, data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        time_range (array-like): some array with 2 strings\n",
    "        data (dict)\n",
    "    \"\"\"\n",
    "    if time_range[0] == \"START\":\n",
    "        t_start = data[\"timerng\"][0]\n",
    "    else:\n",
    "        try:\n",
    "            t_start = int(time_range[0])\n",
    "        except ValueError:\n",
    "            t_start = np.datetime64(time_range[0], \"h\")\n",
    "\n",
    "    if time_range[1] == \"END\":\n",
    "        t_end = data[\"timerng\"][1]\n",
    "    else:\n",
    "        try:\n",
    "            t_end = int(time_range[1])\n",
    "        except ValueError:\n",
    "            t_end = np.datetime64(time_range[1], \"h\")\n",
    "            \n",
    "    if isinstance(t_start, int) and isinstance(t_end, int):\n",
    "        raise TypeError(\"Must have at least one date in the time range.\")\n",
    "    if isinstance(t_start, int):\n",
    "        t_start = t_end - np.timedelta64(t_start, \"h\")\n",
    "    if isinstance(t_end, int):\n",
    "        t_end = t_start + np.timedelta64(t_end, \"h\")\n",
    "        \n",
    "    return t_start, t_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParticleSet and spawn point setup\n",
    "\n",
    "note about interpolation methods: only `linear` works if you want to use the FieldSet in a ParticleSet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "part_path = create_path(\"particledata\")\n",
    "\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    cfg[\"spawn_points\"] = np.array(cfg[\"spawn_points\"])\n",
    "    \n",
    "    if \"time_range\" not in cfg:\n",
    "        t_start = f[\"timerng_secs\"][0]\n",
    "        t_end = f[\"timerng_secs\"][1]\n",
    "    else:\n",
    "        t_start, t_end = parse_time_range(cfg[\"time_range\"], f)\n",
    "        t_start = (t_start - f[\"timerng\"][0]) / np.timedelta64(1, \"s\")\n",
    "        t_end = (t_end - f[\"timerng\"][0]) / np.timedelta64(1, \"s\")\n",
    "            \n",
    "    if cfg[\"repeat_dt\"] <= 0:\n",
    "        repetitions = 1\n",
    "    else:\n",
    "        repetitions = int((t_end - t_start) / cfg[\"repeat_dt\"])\n",
    "    cfg[\"sim_start_sec\"] = t_start\n",
    "    cfg[\"sim_end_sec\"] = t_end\n",
    "    # the total number of particles that will exist in the simulation\n",
    "    total = repetitions * cfg[\"particles_per_dt\"]\n",
    "    lat_arr = np.zeros(total)\n",
    "    lon_arr = np.zeros(total)\n",
    "    time_arr = np.zeros(total)\n",
    "    for i in range(repetitions):\n",
    "        time_arr[cfg[\"particles_per_dt\"] * i:cfg[\"particles_per_dt\"] * (i + 1)] = t_start + cfg[\"repeat_dt\"] * i\n",
    "\n",
    "    # randomly select spawn points from the given config\n",
    "    sp_lat = cfg[\"spawn_points\"].T[0][np.random.randint(0, len(cfg[\"spawn_points\"]), total)]\n",
    "    sp_lon = cfg[\"spawn_points\"].T[1][np.random.randint(0, len(cfg[\"spawn_points\"]), total)]\n",
    "    # vary spawn locations using max_variation\n",
    "    p_lats = add_noise(sp_lat, cfg[\"max_variation\"])\n",
    "    p_lons = add_noise(sp_lon, cfg[\"max_variation\"])\n",
    "\n",
    "    # set up ParticleSet and ParticleFile\n",
    "    f[\"pset\"] = ParticleSet(fieldset=f[\"fs\"], pclass=TimedParticle, lon=p_lons, lat=p_lats, time=time_arr)\n",
    "    save_path = f\"{part_path}/particle_{f['name']}.nc\"\n",
    "    f[\"pfile\"] = f[\"pset\"].ParticleFile(save_path)\n",
    "    print(f\"Particle trajectories for {f['name']} will be saved to {save_path}\")\n",
    "    print(f\"    total particles in simulation: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f[\"pset\"].show(field=\"vector\", vmax=max_v) for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulation setup and execution\n",
    "\n",
    "simulation parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting up times, intervals, and paths for simulation\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    cfg[\"snap_num\"] = math.floor((cfg[\"sim_end_sec\"] - cfg[\"sim_start_sec\"]) / cfg[\"snapshot_interval\"])\n",
    "    cfg[\"last_int\"] = cfg[\"sim_end_sec\"] - (cfg[\"snap_num\"] * cfg[\"snapshot_interval\"] + cfg[\"sim_start_sec\"])\n",
    "    if cfg[\"last_int\"] == 0:\n",
    "        print(\"No last interval exists.\")\n",
    "        print(f\"Num snapshots to save for {f['path']}: {cfg['snap_num'] + 1}\")\n",
    "    else:\n",
    "        print(f\"Num snapshots to save for {f['path']}: {cfg['snap_num'] + 2}\")\n",
    "    if cfg[\"snap_num\"] >= 200:\n",
    "        raise Exception(f\"Too many snapshots ({cfg['snap_num']}).\")\n",
    "    f[\"snap_path\"] = create_path(f\"snapshots/{utils.filename_dict[f['res']]}/{f['name']}\")\n",
    "    print(f\"Path to save snapshots to: {f['snap_path']}\")\n",
    "    # only clear directory if desired or actually saving images\n",
    "    if cfg[\"save_snapshots\"]:\n",
    "        for p in f[\"snap_path\"].glob(\"*.png\"):\n",
    "            p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "execution of all simulation configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show age or not (temporary)\n",
    "age = False\n",
    "# execution of simulation\n",
    "for f in files:\n",
    "    cfg = f[\"cfg\"]\n",
    "    for i in range(cfg[\"snap_num\"]):\n",
    "        exec_save_pset(f, i, cfg[\"snapshot_interval\"], cfg[\"simulation_dt\"], ages=age, save_snapshot=cfg[\"save_snapshots\"])\n",
    "\n",
    "    # save the second-to-last frame\n",
    "    exec_save_pset(f, cfg[\"snap_num\"], 0, 0, ages=age, save_snapshot=cfg[\"save_snapshots\"], exec_pset=False)\n",
    "\n",
    "    # run the last interval (the remainder) if needed\n",
    "    if cfg[\"last_int\"] != 0:\n",
    "        exec_save_pset(f, cfg[\"snap_num\"] + 1, cfg[\"last_int\"], cfg[\"simulation_dt\"], ages=age, save_snapshot=cfg[\"save_snapshots\"])\n",
    "        \n",
    "    # run one more time with 0 runtime because for some reason\n",
    "    # particles can be out-of-bounds without being deleted\n",
    "    # ??? how does this happen ???\n",
    "    # im not actually sure if this really does anything but hey its worth a shot\n",
    "    exec_save_pset(f, 0, 0, 0, save_snapshot=False)\n",
    "        \n",
    "    f[\"pfile\"].export()\n",
    "    f[\"pfile\"].close()\n",
    "\n",
    "print(\"all simulations done and snapshots saved (if simulation was saving snapshots)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gif generation\n",
    "\n",
    "don't have to run, requires [magick](https://imagemagick.org/index.php)\n",
    "\n",
    "the gifs will be saved `snapshots/west_coast_xkm_hourly/` where xkm is the resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "gif_delay = 25 # ms\n",
    "\n",
    "for f in files:\n",
    "    if f[\"cfg\"][\"save_snapshots\"]:\n",
    "        magick_sp = subprocess.Popen(\n",
    "            [\n",
    "                \"magick\", \"-delay\", str(gif_delay),\n",
    "                str(f[\"snap_path\"]) + \"/*.png\", # path to the snapshots to stitch\n",
    "                f\"snapshots/{utils.filename_dict[f['res']]}/partsim_{f['name']}.gif\" # path to save gif to\n",
    "            ],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True\n",
    "        )\n",
    "        stdout, stderr = magick_sp.communicate()\n",
    "        print((stdout, stderr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_parcels",
   "language": "python",
   "name": "py3_parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
