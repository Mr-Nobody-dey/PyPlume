{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# missing data interpolation\n",
    "\n",
    "statistics is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### potential shenanigans\n",
    "\n",
    "\"Several techniques have been used to fill the gaps in either the UWLS or OI derived total vector maps.\n",
    "\n",
    "These are implemented using covariance derived from normal mode analysis (Lipphardt et al. 2000), open-boundary modal analysis (OMA) (Kaplan and Lekien 2007), and empirical orthogonal function (EOF) analysis (Beckers and Rixen 2003; Alvera-Azc√°rate et al. 2005); and using idealized or smoothed observed covariance (Davis 1985).\"\n",
    "\n",
    "- normal mode analysis\n",
    "- open-boundary modal analysis (OMA)\n",
    "- empirical orthogonal function analysis (EOF)\n",
    "- use idealized/smoothed observed covariance\n",
    "\n",
    "---\n",
    "\n",
    "### other ideas\n",
    "\n",
    "DINEOF (could only find an implementation in R)\n",
    "\n",
    "to be honest I don't understand any of these methods but they look cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### currently implemented:\n",
    "\n",
    "rip data straight from the lower resolution data for areas where data is considered missing in the high resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from parcels import FieldSet\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import utils\n",
    "from parcels_utils import xr_dataset_to_fieldset, HFRGrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target and interp_references\n",
    "\n",
    "`target` is the data you are interpolating.\n",
    "\n",
    "`interp_references` is a list of reference data to interpolate from. A few specifications:\n",
    "- should be ordered from most accurate data to least accurate (highest to lowest resolution)\n",
    "- time domain should be identical or bigger than the one of the target\n",
    "- lat and lon domain should be bigger than the target's to prevent any out-of-bounds complications\n",
    "\n",
    "`mask_nc` must have the exact same lat and lon dimensions of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_root = Path(\"/Volumes/T7/Documents/Programs/scripps-cordc/parcels_westcoast/current_netcdfs\")\n",
    "\n",
    "target = HFRGrid(files_root / \"west_coast_1km_hourly/tj_plume_2020-08.nc\")\n",
    "\n",
    "interp_references = [\n",
    "    HFRGrid(files_root / \"west_coast_2km_hourly/tj_plume_2020-08.nc\"),\n",
    "    HFRGrid(files_root / \"west_coast_6km_hourly/tj_plume_2020-08.nc\"),\n",
    "]\n",
    "\n",
    "mask_nc = HFRGrid(files_root / \"west_coast_1km_hourly/tj_sample.nc\", init_fs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check validity of interpolation references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_grids(target, references, mask):\n",
    "    targ_times, targ_lats, targ_lons = target.get_coords()\n",
    "    targ_min = (targ_lats[0], targ_lons[0])\n",
    "    targ_max = (targ_lats[-1], targ_lons[-1])\n",
    "    # check references\n",
    "    for ref in references:\n",
    "        ref_times, ref_lats, ref_lons = ref.get_coords()\n",
    "        lat_inbounds = (ref_lats[0] <= targ_min[0]) and (ref_lats[-1] >= targ_max[0])\n",
    "        lon_inbounds = (ref_lons[0] <= targ_min[1]) and (ref_lons[-1] >= targ_max[1])\n",
    "        time_inbounds = (ref_times[0] <= targ_times[0]) and (ref_times[-1] >= targ_times[-1])\n",
    "        if not (lat_inbounds and lon_inbounds and time_inbounds):\n",
    "            raise ValueError(\"Incorrect reference dimensions\")\n",
    "    # check mask\n",
    "    _, mask_lats, mask_lons = mask.get_coords()\n",
    "    mask_same_res = (len(targ_lats) == len(mask_lats)) and (len(targ_lons) == len(mask_lons))\n",
    "    if not mask_same_res:\n",
    "        raise ValueError(\"Mask is not the same lat/lon shape as target\")\n",
    "    # mask_nc should just be sliced before being used\n",
    "    # change these asserts to >= later when that's done\n",
    "    lat_inbounds = (mask_lats[0] == targ_min[0]) and (mask_lats[-1] == targ_max[0])\n",
    "    lon_inbounds = (mask_lons[0] == targ_min[1]) and (mask_lons[-1] == targ_max[1])\n",
    "    if not (lat_inbounds and lon_inbounds):\n",
    "        raise ValueError(\"Incorrect mask dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_grids(target, interp_references, mask_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolation type\n",
    "\n",
    "more information can be found in the `tutorial_interpolation` notebook\n",
    "\n",
    "EDIT: just use `linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan values and parcels\n",
    "\n",
    "note that when this xarray Dataset is passed into parcels, all the nan values change to 0 and the mask generation won't work anymore\n",
    "\n",
    "so the Dataset is copied for use with the FieldSet instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_data = utils.generate_mask_none(mask_nc.xrds[\"u\"].values)\n",
    "no_data = np.tile(no_data, (target.xrds[\"time\"].size, 1, 1))\n",
    "invalid = utils.generate_mask_invalid(target.xrds[\"u\"].values)\n",
    "invalid_pos = np.where(invalid)\n",
    "num_invalid = invalid.sum()\n",
    "print(f\"total invalid values on target data: {num_invalid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use of Parcels Field for interpolation\n",
    "\n",
    "indexing Field values goes [time, depth, lat, lon]\n",
    "\n",
    "Field does linear interpolation automatically when indexing values between it's coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interped(i, ref, invalid_where):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        i (int): index on invalid_where\n",
    "        ref (HFRGrid): reference Dataset\n",
    "        invalid_where (array-like): (3, n) dimensional array representing all invalid positions\n",
    "    \n",
    "    Returns:\n",
    "        (u, v): (nan, nan) if no data was found, interpolated values otherwise\n",
    "    \"\"\"\n",
    "    time_diff = np.diff(ref.fieldset_flat.U.grid.time)[0]\n",
    "    t = invalid_where[0][i]\n",
    "    lat = target.lats[invalid_where[1][i]]\n",
    "    lon = target.lons[invalid_where[2][i]]\n",
    "    current_u, current_v = ref.get_fs_current(t * time_diff, lat, lon)\n",
    "    current_abs = abs(current_u) + abs(current_v)\n",
    "    # if both the u and v components are 0, there's probably no data there\n",
    "    if np.isnan(ref.get_closest_current(t, lat, lon)[0]) or current_abs == 0:\n",
    "        return np.nan, np.nan\n",
    "    return current_u, current_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear interpolation using lower resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_interped_u = target.xrds[\"u\"].values.copy()\n",
    "target_interped_v = target.xrds[\"v\"].values.copy()\n",
    "invalid_interped = invalid.copy()\n",
    "for f in interp_references:\n",
    "    invalid_pos_new = np.where(invalid_interped)\n",
    "    num_invalid_new = invalid_interped.sum()\n",
    "    arr_u = np.zeros(num_invalid_new)\n",
    "    arr_v = np.zeros(num_invalid_new)\n",
    "    print(f\"Attempting to interpolate {num_invalid_new} points...\")\n",
    "    for i in range(num_invalid_new):\n",
    "        c_u, c_v = get_interped(i, f, invalid_pos_new)\n",
    "        arr_u[i] = c_u\n",
    "        arr_v[i] = c_v\n",
    "    target_interped_u[invalid_pos_new] = arr_u\n",
    "    target_interped_v[invalid_pos_new] = arr_v\n",
    "    invalid_interped = utils.generate_mask_invalid(target_interped_u)\n",
    "    print(f\"total invalid values after interpolation with {f}: {invalid_interped.sum()}\")\n",
    "    print(f\"    values filled: {num_invalid_new - invalid_interped.sum()}\")\n",
    "print(f\"total invalid values on interpolated: {invalid_interped.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## even more filling with PLS and smoothing with DCT shenanigans\n",
    "\n",
    "uses the matlab engine and smoothn.m\n",
    "\n",
    "https://www.mathworks.com/help/matlab/matlab-engine-for-python.html\n",
    "\n",
    "https://www.mathworks.com/matlabcentral/fileexchange/25634-smoothn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "\n",
    "eng = matlab.engine.start_matlab()\n",
    "\n",
    "target_smoothed_u = target_interped_u.copy()\n",
    "target_smoothed_v = target_interped_v.copy()\n",
    "u_list = target_smoothed_u.tolist()\n",
    "v_list = target_smoothed_v.tolist()\n",
    "\n",
    "print(f\"Filling {len(u_list)} fields...\")\n",
    "for i in range(len(u_list)):\n",
    "    u_mat = matlab.double(u_list[i])\n",
    "    v_mat = matlab.double(v_list[i])\n",
    "    uv_smooth = eng.smoothn([u_mat, v_mat], \"robust\")\n",
    "    u_array = np.empty(uv_smooth[0].size)\n",
    "    v_array = np.empty(uv_smooth[1].size)\n",
    "    u_array[:] = uv_smooth[0]\n",
    "    v_array[:] = uv_smooth[1]\n",
    "    target_smoothed_u[i] = u_array\n",
    "    target_smoothed_v[i] = v_array\n",
    "\n",
    "# mask the filled data\n",
    "target_smoothed_u[no_data] = np.nan\n",
    "target_smoothed_v[no_data] = np.nan\n",
    "\n",
    "eng.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### formatting and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-add coordinates, dimensions, and metadata to interpolated data\n",
    "darr_u = utils.conv_to_dataarray(target_smoothed_u, target.xrds[\"u\"])\n",
    "darr_v = utils.conv_to_dataarray(target_smoothed_v, target.xrds[\"v\"])\n",
    "target_interped_xrds = target.xrds.drop_vars([\"u\", \"v\"]).assign(u=darr_u, v=darr_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = str(target.path).split(\".nc\")[0] + \"_interped.nc\"\n",
    "target_interped_xrds.to_netcdf(save_path)\n",
    "print(f\"saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display field to see if interpolation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_interp = xr_dataset_to_fieldset(target_interped_xrds)\n",
    "target.fieldset.U.show()  # uninterpolated\n",
    "fs_interp.U.show()  # interpolated, gapfilled, smoothed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-parcels",
   "language": "python",
   "name": "py3-parcels"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
